{
    "comment": "Only include flexpipe related configurations",
    "num_layers": 2,
    "num_stages": 2,
    "num_gpus": [
        4,
        4
    ],
    "flex_recompute_activations": [
        true,
        false
    ],
    "resharding_stages": [
        false,
        false
    ],
    "num_ops_in_each_stage": [
        3,
        3
    ],
    "tensor_parallel_size_of_each_op": [
        [
            4,
            4,
            4
        ],
        [
            4,
            4,
            4
        ]
    ],
    "data_parallel_size_of_each_op": [
        [
            1,
            1,
            1
        ],
        [
            1,
            1,
            1
        ]
    ],
    "recompute_ops": [
        [
            false,
            true,
            true
        ],
        [
            true,
            true,
            false
        ]
    ],
    "algo_of_each_op": [
        [
            0,
            0,
            0
        ],
        [
            0,
            0,
            0
        ]
    ]
}


GPT_ARGS="
    --use-mcore-models \--
    --tokenizer-type GPT2BPETokenizer \
    --use-flash-attn \
    --use-distributed-optimizer \
    --recompute-granularity selective \++++++
    --train-iters 10 \
    --weight-decay 0.1 \
    --initial-loss-scale 4096 \
    --clip-grad 1.0 \
    --lr 6.0e-5 \
    --lr-decay-style cosine \
    --min-lr 6.0e-6 \
    --lr-warmup-fraction .001 \
    --lr-decay-iters 430000 \
    --no-gradient-accumulation-fusion \
    --no-masked-softmax-fusion \
    --no-bias-gelu-fusion \
    --attention-softmax-in-fp32 \
    --attention-dropout 0.0 \
    --hidden-dropout 0.0 \
    --no-shared-storage \
    --bf16
"


args.model_name = config_dict["model_name"]
args.model_size = config_dict["model_size"]
args.num_layers = config_dict["num_layers"]
args.seq_length = config_dict["seq_length"]
args.max_position_embeddings = config_dict["max_position_embeddings"]
args.num_attention_heads = config_dict["num_attention_heads"]
args.hidden_size = config_dict["hidden_size"]
args.global_batch_size = config_dict["global_batch_size"]
args.micro_batch_size = config_dict["micro_batch_size"]
args.num_stages = config_dict["num_stages"]



def load_json_args(json_file, args):
    with open(json_file) as f:
        config_dict = json.load(f)
        args.model_name = config_dict["model_name"]
        args.model_size = config_dict["model_size"]
        args.num_layers = config_dict["num_layers"]
        args.seq_length = config_dict["seq_length"]
        args.max_position_embeddings = config_dict["max_position_embeddings"]
        args.num_attention_heads = config_dict["num_attention_heads"]
        args.hidden_size = config_dict["hidden_size"]
        args.global_batch_size = config_dict["global_batch_size"]
        args.micro_batch_size = config_dict["micro_batch_size"]
        args.num_stages = config_dict["num_stages"]
        args.num_gpus = config_dict["num_gpus"]
        args.checkpoint_activations = config_dict["checkpoint_activations"]
        # args.flex_recompute_activations = config_dict["flex_recompute_activations"]
        args.resharding_stages = config_dict["resharding_stages"]
        args.num_ops_in_each_stage = config_dict["num_ops_in_each_stage"]
        args.tensor_parallel_size_of_each_op = config_dict["model_parallel_size_of_each_op"]
        args.data_parallel_size_of_each_op = config_dict["data_parallel_size_of_each_op"]
        args.recompute_ops = config_dict["recompute_ops"]
        args.algo_of_each_op = config_dict["algo_of_each_op"]
        args.pipeline_model_parallel_size = args.num_stages
    return args