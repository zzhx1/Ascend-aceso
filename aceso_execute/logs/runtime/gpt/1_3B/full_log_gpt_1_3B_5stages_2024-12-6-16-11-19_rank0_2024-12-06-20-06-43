[2024-12-06 20:06:45,933] torch.distributed.run: [WARNING] 
[2024-12-06 20:06:45,933] torch.distributed.run: [WARNING] *****************************************
[2024-12-06 20:06:45,933] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-06 20:06:45,933] torch.distributed.run: [WARNING] *****************************************
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:299: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module adaptive_cp...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
ninja: no work to do.
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module adaptive_cp...
Namespace(num_layers=24, encoder_num_layers=None, decoder_num_layers=None, hidden_size=2048, ffn_hidden_size=None, num_attention_heads=32, kv_channels=None, group_query_attention=False, num_query_groups=1, max_position_embeddings=2048, position_embedding_type='learned_absolute', use_rotary_position_embeddings=False, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='LayerNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=None, end_weight_decay=None, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=2, batch_size=None, global_batch_size=1024, rampup_batch_size=None, recompute_activations=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=None, empty_unused_memory_level=0, checkpoint_activations=[False, False, False, False, False], train_iters=3, train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, use_flash_attn=True, add_bias_linear=True, optimizer='adam', dataloader_type=None, async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, use_mcore_models=True, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=320000, lr_decay_samples=None, lr_warmup_fraction=0.001, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, warmup=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save=None, save_interval=None, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save=False, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=5, pipeline_model_parallel_split_rank=None, model_parallel_size=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', overlap_grad_reduce=False, delay_grad_reduce=True, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=True, context_parallel_size=1, nccl_communicator_config_path=None, eval_iters=0, eval_interval=1000, test_mode=False, skip_train=False, data_path=None, split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=None, vocab_file='vocabs_file/gpt2-vocab.json', merge_file='vocabs_file/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=None, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_grouped_gemm=False, moe_input_jitter_eps=None, moe_token_dropping=False, moe_per_layer_logging=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2e-tracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, yaml_cfg=None, flexpipe=True, flexpipe_config='/workspace/RC4/ModelLink/aceso_execute/logs/configs/gpt/1_3B/top_configs/gpt_1_3B_5stages_2024-12-6-16-11-19.json', interleave_factor=1, checkpoint_stages=[], log_path='/workspace/RC4/ModelLink/aceso_execute/logs/runtime/gpt/1_3B/', prof_op=False, prof_tp_size=None, prof_path=None, prof_cache_file=None, prof_model_name='all', prof_model_size='all', prof_time_only=False, prof_memory_only=False, prof_warmup_times=20, prof_repeat_times=[50], prof_warmup_threshold=None, prof_repeat_threshold=None, prof_skip_running=False, prof_num_nodes=None, prof_node_rank=None, prof_ref_data=None, prof_mbs_list=None, use_fused_rmsnorm=False, use_fused_swiglu=False, use_fused_rotary_pos_emb=False, use_fused_ring_attention_update=False, use_mc2=False, padded_vocab_size=None, embed_layernorm=False, use_glm_rope=False, sliding_window=None, output_layer_slice_num=1, lora_target_modules=[], lora_load=None, lora_r=16, lora_alpha=32, lora_modules_to_save=None, lora_register_forward_hook=['word_embeddings', 'input_layernorm'], lora_fusion=False, is_instruction_dataset=False, full_shuffle_instruction_dataset=False, variable_seq_lengths=False, tokenizer_kwargs=None, tokenizer_padding_side='right', tokenizer_type='GPT2BPETokenizer', tokenizer_name_or_path=None, tokenizer_not_use_fast=True, input_layernorm_in_fp32=False, no_shuffle=False, moe_router_topk=2, moe_router_load_balancing_type='aux_loss', expert_interval=1, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=0.0, moe_train_capacity_factor=1.0, use_fused_moe_token_permute_and_unpermute=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_token_dispatcher_type='allgather', noisy_gate_policy=None, enable_token_rearrange_opt=False, embedding_multiplier_scale=1.0, input_jitter=True, post_norm=False, output_multiplier_scale=None, moe_permutation_async_comm=False, shared_expert_gate=False, shared_expert_gate_output_dimension=1, num_layer_list=None, profile_ranks=[-1], profile_level='level0', profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_with_cpu=False, profile_save_path='./profile_dir', add_qkv_bias=False, add_dense_bias=False, skip_bias_add=True, add_rmsnorm_offset=False, geglu=False, input_embeds_norm=False, gelu_tanh=False, output_logit_softcapping=None, attn_logit_softcapping=None, query_pre_attn_scalar=None, interleave_sliding_window=None, stage=None, gradient_accumulation_fusion=False, transformer_impl='local', enable_recompute_layers_per_pp_rank=False, pre_tockens=65536, next_tockens=0, sparse_mode=0, shape_order='SBH', use_deter_comp=False, jit_compile=False, prompt_type=None, pad_to_multiple_of=8, scale_emb=None, dim_model_base=None, no_cut_token=False, scale_depth=None, swap_attention=False, swap_modules=None, load_checkpoint_loosely=False, no_post_layer_norm=False, distributed_timeout_minutes=45, rotary_base=None, rope_scaling_type=None, low_freq_factor=None, high_freq_factor=None, original_max_position_embeddings=None, reuse_fp32_param=False, recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_in_advance=False, square_alibi_mask=False, fill_neg_inf=False, no_shared_storage=True, enable_high_availability=False, enable_optimizer_state_local_copy=False, enable_hbmfault_repair=False, context_parallel_algo='ulysses_cp_algo', ulysses_degree_in_cp=None, cp_attention_mask_type='causal', use_cp_send_recv_overlap=False, cp_window_size=1, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, kv_head_repeat_before_uly_alltoall=True, multi_head_latent_attention=False, q_lora_rank=None, kv_lora_rank=None, v_head_dim=None, qk_rope_head_dim=None, qk_nope_head_dim=None, rope_scaling_beta_fast=32, rope_scaling_beta_slow=1, rope_scaling_factor=1.0, rope_scaling_mscale=1.0, rope_scaling_mscale_all_dim=0.0, rope_scaling_original_max_position_embeddings=None, moe_intermediate_size=None, n_shared_experts=None, topk_group=None, routed_scaling_factor=None, norm_topk_prob=False, seq_aux=False, first_k_dense_replace=None, moe_layer_freq=None, moe_device_level_aux_loss_coeff=0.0, moe_comm_aux_loss_coeff=0.0, dpo_beta=0.1, dpo_loss_type='sigmoid', dpo_ftx=0.0, ref_model=None, dpo_label_smoothing=0.0, pref_ftx=0.0, is_pairwise_dataset=False, rank=0, world_size=8, log_name='gpt_1_3B_5stages_2024-12-6-16-11-19', num_stages=5, num_gpus=[1, 1, 2, 2, 2], resharding_stages=[False, False, False, False, False], num_ops_in_each_stage=[10, 9, 18, 18, 18], tensor_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], data_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], recompute_ops=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], algo_of_each_op=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
world_size: 8
using world size: 8, context-parallel size: 1
--checkpoint-activations is no longer valid, use --recompute-activations, or, for more control, --recompute-granularity and --recompute-method.
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 60, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:06:54 (PID:241634, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp9bo6auwy'>
  _warnings.warn(warn_message, ResourceWarning)
Namespace(num_layers=24, encoder_num_layers=None, decoder_num_layers=None, hidden_size=2048, ffn_hidden_size=None, num_attention_heads=32, kv_channels=None, group_query_attention=False, num_query_groups=1, max_position_embeddings=2048, position_embedding_type='learned_absolute', use_rotary_position_embeddings=False, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='LayerNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=None, end_weight_decay=None, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=2, batch_size=None, global_batch_size=1024, rampup_batch_size=None, recompute_activations=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=None, empty_unused_memory_level=0, checkpoint_activations=[False, False, False, False, False], train_iters=3, train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, use_flash_attn=True, add_bias_linear=True, optimizer='adam', dataloader_type=None, async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, use_mcore_models=True, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=320000, lr_decay_samples=None, lr_warmup_fraction=0.001, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, warmup=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save=None, save_interval=None, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save=False, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=5, pipeline_model_parallel_split_rank=None, model_parallel_size=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', overlap_grad_reduce=False, delay_grad_reduce=True, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=True, context_parallel_size=1, nccl_communicator_config_path=None, eval_iters=0, eval_interval=1000, test_mode=False, skip_train=False, data_path=None, split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=None, vocab_file='vocabs_file/gpt2-vocab.json', merge_file='vocabs_file/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=None, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_grouped_gemm=False, moe_input_jitter_eps=None, moe_token_dropping=False, moe_per_layer_logging=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2e-tracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, yaml_cfg=None, flexpipe=True, flexpipe_config='/workspace/RC4/ModelLink/aceso_execute/logs/configs/gpt/1_3B/top_configs/gpt_1_3B_5stages_2024-12-6-16-11-19.json', interleave_factor=1, checkpoint_stages=[], log_path='/workspace/RC4/ModelLink/aceso_execute/logs/runtime/gpt/1_3B/', prof_op=False, prof_tp_size=None, prof_path=None, prof_cache_file=None, prof_model_name='all', prof_model_size='all', prof_time_only=False, prof_memory_only=False, prof_warmup_times=20, prof_repeat_times=[50], prof_warmup_threshold=None, prof_repeat_threshold=None, prof_skip_running=False, prof_num_nodes=None, prof_node_rank=None, prof_ref_data=None, prof_mbs_list=None, use_fused_rmsnorm=False, use_fused_swiglu=False, use_fused_rotary_pos_emb=False, use_fused_ring_attention_update=False, use_mc2=False, padded_vocab_size=None, embed_layernorm=False, use_glm_rope=False, sliding_window=None, output_layer_slice_num=1, lora_target_modules=[], lora_load=None, lora_r=16, lora_alpha=32, lora_modules_to_save=None, lora_register_forward_hook=['word_embeddings', 'input_layernorm'], lora_fusion=False, is_instruction_dataset=False, full_shuffle_instruction_dataset=False, variable_seq_lengths=False, tokenizer_kwargs=None, tokenizer_padding_side='right', tokenizer_type='GPT2BPETokenizer', tokenizer_name_or_path=None, tokenizer_not_use_fast=True, input_layernorm_in_fp32=False, no_shuffle=False, moe_router_topk=2, moe_router_load_balancing_type='aux_loss', expert_interval=1, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=0.0, moe_train_capacity_factor=1.0, use_fused_moe_token_permute_and_unpermute=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_token_dispatcher_type='allgather', noisy_gate_policy=None, enable_token_rearrange_opt=False, embedding_multiplier_scale=1.0, input_jitter=True, post_norm=False, output_multiplier_scale=None, moe_permutation_async_comm=False, shared_expert_gate=False, shared_expert_gate_output_dimension=1, num_layer_list=None, profile_ranks=[-1], profile_level='level0', profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_with_cpu=False, profile_save_path='./profile_dir', add_qkv_bias=False, add_dense_bias=False, skip_bias_add=True, add_rmsnorm_offset=False, geglu=False, input_embeds_norm=False, gelu_tanh=False, output_logit_softcapping=None, attn_logit_softcapping=None, query_pre_attn_scalar=None, interleave_sliding_window=None, stage=None, gradient_accumulation_fusion=False, transformer_impl='local', enable_recompute_layers_per_pp_rank=False, pre_tockens=65536, next_tockens=0, sparse_mode=0, shape_order='SBH', use_deter_comp=False, jit_compile=False, prompt_type=None, pad_to_multiple_of=8, scale_emb=None, dim_model_base=None, no_cut_token=False, scale_depth=None, swap_attention=False, swap_modules=None, load_checkpoint_loosely=False, no_post_layer_norm=False, distributed_timeout_minutes=45, rotary_base=None, rope_scaling_type=None, low_freq_factor=None, high_freq_factor=None, original_max_position_embeddings=None, reuse_fp32_param=False, recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_in_advance=False, square_alibi_mask=False, fill_neg_inf=False, no_shared_storage=True, enable_high_availability=False, enable_optimizer_state_local_copy=False, enable_hbmfault_repair=False, context_parallel_algo='ulysses_cp_algo', ulysses_degree_in_cp=None, cp_attention_mask_type='causal', use_cp_send_recv_overlap=False, cp_window_size=1, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, kv_head_repeat_before_uly_alltoall=True, multi_head_latent_attention=False, q_lora_rank=None, kv_lora_rank=None, v_head_dim=None, qk_rope_head_dim=None, qk_nope_head_dim=None, rope_scaling_beta_fast=32, rope_scaling_beta_slow=1, rope_scaling_factor=1.0, rope_scaling_mscale=1.0, rope_scaling_mscale_all_dim=0.0, rope_scaling_original_max_position_embeddings=None, moe_intermediate_size=None, n_shared_experts=None, topk_group=None, routed_scaling_factor=None, norm_topk_prob=False, seq_aux=False, first_k_dense_replace=None, moe_layer_freq=None, moe_device_level_aux_loss_coeff=0.0, moe_comm_aux_loss_coeff=0.0, dpo_beta=0.1, dpo_loss_type='sigmoid', dpo_ftx=0.0, ref_model=None, dpo_label_smoothing=0.0, pref_ftx=0.0, is_pairwise_dataset=False, rank=7, world_size=8, log_name='gpt_1_3B_5stages_2024-12-6-16-11-19', num_stages=5, num_gpus=[1, 1, 2, 2, 2], resharding_stages=[False, False, False, False, False], num_ops_in_each_stage=[10, 9, 18, 18, 18], tensor_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], data_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], recompute_ops=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], algo_of_each_op=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 60, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:06:54 (PID:241641, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp_q6fmjzo'>
  _warnings.warn(warn_message, ResourceWarning)
Namespace(num_layers=24, encoder_num_layers=None, decoder_num_layers=None, hidden_size=2048, ffn_hidden_size=None, num_attention_heads=32, kv_channels=None, group_query_attention=False, num_query_groups=1, max_position_embeddings=2048, position_embedding_type='learned_absolute', use_rotary_position_embeddings=False, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='LayerNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=None, end_weight_decay=None, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=2, batch_size=None, global_batch_size=1024, rampup_batch_size=None, recompute_activations=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=None, empty_unused_memory_level=0, checkpoint_activations=[False, False, False, False, False], train_iters=3, train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, use_flash_attn=True, add_bias_linear=True, optimizer='adam', dataloader_type=None, async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, use_mcore_models=True, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=320000, lr_decay_samples=None, lr_warmup_fraction=0.001, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, warmup=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save=None, save_interval=None, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save=False, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=5, pipeline_model_parallel_split_rank=None, model_parallel_size=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', overlap_grad_reduce=False, delay_grad_reduce=True, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=True, context_parallel_size=1, nccl_communicator_config_path=None, eval_iters=0, eval_interval=1000, test_mode=False, skip_train=False, data_path=None, split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=None, vocab_file='vocabs_file/gpt2-vocab.json', merge_file='vocabs_file/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=None, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_grouped_gemm=False, moe_input_jitter_eps=None, moe_token_dropping=False, moe_per_layer_logging=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2e-tracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, yaml_cfg=None, flexpipe=True, flexpipe_config='/workspace/RC4/ModelLink/aceso_execute/logs/configs/gpt/1_3B/top_configs/gpt_1_3B_5stages_2024-12-6-16-11-19.json', interleave_factor=1, checkpoint_stages=[], log_path='/workspace/RC4/ModelLink/aceso_execute/logs/runtime/gpt/1_3B/', prof_op=False, prof_tp_size=None, prof_path=None, prof_cache_file=None, prof_model_name='all', prof_model_size='all', prof_time_only=False, prof_memory_only=False, prof_warmup_times=20, prof_repeat_times=[50], prof_warmup_threshold=None, prof_repeat_threshold=None, prof_skip_running=False, prof_num_nodes=None, prof_node_rank=None, prof_ref_data=None, prof_mbs_list=None, use_fused_rmsnorm=False, use_fused_swiglu=False, use_fused_rotary_pos_emb=False, use_fused_ring_attention_update=False, use_mc2=False, padded_vocab_size=None, embed_layernorm=False, use_glm_rope=False, sliding_window=None, output_layer_slice_num=1, lora_target_modules=[], lora_load=None, lora_r=16, lora_alpha=32, lora_modules_to_save=None, lora_register_forward_hook=['word_embeddings', 'input_layernorm'], lora_fusion=False, is_instruction_dataset=False, full_shuffle_instruction_dataset=False, variable_seq_lengths=False, tokenizer_kwargs=None, tokenizer_padding_side='right', tokenizer_type='GPT2BPETokenizer', tokenizer_name_or_path=None, tokenizer_not_use_fast=True, input_layernorm_in_fp32=False, no_shuffle=False, moe_router_topk=2, moe_router_load_balancing_type='aux_loss', expert_interval=1, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=0.0, moe_train_capacity_factor=1.0, use_fused_moe_token_permute_and_unpermute=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_token_dispatcher_type='allgather', noisy_gate_policy=None, enable_token_rearrange_opt=False, embedding_multiplier_scale=1.0, input_jitter=True, post_norm=False, output_multiplier_scale=None, moe_permutation_async_comm=False, shared_expert_gate=False, shared_expert_gate_output_dimension=1, num_layer_list=None, profile_ranks=[-1], profile_level='level0', profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_with_cpu=False, profile_save_path='./profile_dir', add_qkv_bias=False, add_dense_bias=False, skip_bias_add=True, add_rmsnorm_offset=False, geglu=False, input_embeds_norm=False, gelu_tanh=False, output_logit_softcapping=None, attn_logit_softcapping=None, query_pre_attn_scalar=None, interleave_sliding_window=None, stage=None, gradient_accumulation_fusion=False, transformer_impl='local', enable_recompute_layers_per_pp_rank=False, pre_tockens=65536, next_tockens=0, sparse_mode=0, shape_order='SBH', use_deter_comp=False, jit_compile=False, prompt_type=None, pad_to_multiple_of=8, scale_emb=None, dim_model_base=None, no_cut_token=False, scale_depth=None, swap_attention=False, swap_modules=None, load_checkpoint_loosely=False, no_post_layer_norm=False, distributed_timeout_minutes=45, rotary_base=None, rope_scaling_type=None, low_freq_factor=None, high_freq_factor=None, original_max_position_embeddings=None, reuse_fp32_param=False, recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_in_advance=False, square_alibi_mask=False, fill_neg_inf=False, no_shared_storage=True, enable_high_availability=False, enable_optimizer_state_local_copy=False, enable_hbmfault_repair=False, context_parallel_algo='ulysses_cp_algo', ulysses_degree_in_cp=None, cp_attention_mask_type='causal', use_cp_send_recv_overlap=False, cp_window_size=1, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, kv_head_repeat_before_uly_alltoall=True, multi_head_latent_attention=False, q_lora_rank=None, kv_lora_rank=None, v_head_dim=None, qk_rope_head_dim=None, qk_nope_head_dim=None, rope_scaling_beta_fast=32, rope_scaling_beta_slow=1, rope_scaling_factor=1.0, rope_scaling_mscale=1.0, rope_scaling_mscale_all_dim=0.0, rope_scaling_original_max_position_embeddings=None, moe_intermediate_size=None, n_shared_experts=None, topk_group=None, routed_scaling_factor=None, norm_topk_prob=False, seq_aux=False, first_k_dense_replace=None, moe_layer_freq=None, moe_device_level_aux_loss_coeff=0.0, moe_comm_aux_loss_coeff=0.0, dpo_beta=0.1, dpo_loss_type='sigmoid', dpo_ftx=0.0, ref_model=None, dpo_label_smoothing=0.0, pref_ftx=0.0, is_pairwise_dataset=False, rank=3, world_size=8, log_name='gpt_1_3B_5stages_2024-12-6-16-11-19', num_stages=5, num_gpus=[1, 1, 2, 2, 2], resharding_stages=[False, False, False, False, False], num_ops_in_each_stage=[10, 9, 18, 18, 18], tensor_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], data_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], recompute_ops=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], algo_of_each_op=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
Namespace(num_layers=24, encoder_num_layers=None, decoder_num_layers=None, hidden_size=2048, ffn_hidden_size=None, num_attention_heads=32, kv_channels=None, group_query_attention=False, num_query_groups=1, max_position_embeddings=2048, position_embedding_type='learned_absolute', use_rotary_position_embeddings=False, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='LayerNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=None, end_weight_decay=None, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=2, batch_size=None, global_batch_size=1024, rampup_batch_size=None, recompute_activations=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=None, empty_unused_memory_level=0, checkpoint_activations=[False, False, False, False, False], train_iters=3, train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, use_flash_attn=True, add_bias_linear=True, optimizer='adam', dataloader_type=None, async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, use_mcore_models=True, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=320000, lr_decay_samples=None, lr_warmup_fraction=0.001, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, warmup=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save=None, save_interval=None, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save=False, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=5, pipeline_model_parallel_split_rank=None, model_parallel_size=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', overlap_grad_reduce=False, delay_grad_reduce=True, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=True, context_parallel_size=1, nccl_communicator_config_path=None, eval_iters=0, eval_interval=1000, test_mode=False, skip_train=False, data_path=None, split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=None, vocab_file='vocabs_file/gpt2-vocab.json', merge_file='vocabs_file/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=None, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_grouped_gemm=False, moe_input_jitter_eps=None, moe_token_dropping=False, moe_per_layer_logging=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2e-tracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, yaml_cfg=None, flexpipe=True, flexpipe_config='/workspace/RC4/ModelLink/aceso_execute/logs/configs/gpt/1_3B/top_configs/gpt_1_3B_5stages_2024-12-6-16-11-19.json', interleave_factor=1, checkpoint_stages=[], log_path='/workspace/RC4/ModelLink/aceso_execute/logs/runtime/gpt/1_3B/', prof_op=False, prof_tp_size=None, prof_path=None, prof_cache_file=None, prof_model_name='all', prof_model_size='all', prof_time_only=False, prof_memory_only=False, prof_warmup_times=20, prof_repeat_times=[50], prof_warmup_threshold=None, prof_repeat_threshold=None, prof_skip_running=False, prof_num_nodes=None, prof_node_rank=None, prof_ref_data=None, prof_mbs_list=None, use_fused_rmsnorm=False, use_fused_swiglu=False, use_fused_rotary_pos_emb=False, use_fused_ring_attention_update=False, use_mc2=False, padded_vocab_size=None, embed_layernorm=False, use_glm_rope=False, sliding_window=None, output_layer_slice_num=1, lora_target_modules=[], lora_load=None, lora_r=16, lora_alpha=32, lora_modules_to_save=None, lora_register_forward_hook=['word_embeddings', 'input_layernorm'], lora_fusion=False, is_instruction_dataset=False, full_shuffle_instruction_dataset=False, variable_seq_lengths=False, tokenizer_kwargs=None, tokenizer_padding_side='right', tokenizer_type='GPT2BPETokenizer', tokenizer_name_or_path=None, tokenizer_not_use_fast=True, input_layernorm_in_fp32=False, no_shuffle=False, moe_router_topk=2, moe_router_load_balancing_type='aux_loss', expert_interval=1, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=0.0, moe_train_capacity_factor=1.0, use_fused_moe_token_permute_and_unpermute=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_token_dispatcher_type='allgather', noisy_gate_policy=None, enable_token_rearrange_opt=False, embedding_multiplier_scale=1.0, input_jitter=True, post_norm=False, output_multiplier_scale=None, moe_permutation_async_comm=False, shared_expert_gate=False, shared_expert_gate_output_dimension=1, num_layer_list=None, profile_ranks=[-1], profile_level='level0', profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_with_cpu=False, profile_save_path='./profile_dir', add_qkv_bias=False, add_dense_bias=False, skip_bias_add=True, add_rmsnorm_offset=False, geglu=False, input_embeds_norm=False, gelu_tanh=False, output_logit_softcapping=None, attn_logit_softcapping=None, query_pre_attn_scalar=None, interleave_sliding_window=None, stage=None, gradient_accumulation_fusion=False, transformer_impl='local', enable_recompute_layers_per_pp_rank=False, pre_tockens=65536, next_tockens=0, sparse_mode=0, shape_order='SBH', use_deter_comp=False, jit_compile=False, prompt_type=None, pad_to_multiple_of=8, scale_emb=None, dim_model_base=None, no_cut_token=False, scale_depth=None, swap_attention=False, swap_modules=None, load_checkpoint_loosely=False, no_post_layer_norm=False, distributed_timeout_minutes=45, rotary_base=None, rope_scaling_type=None, low_freq_factor=None, high_freq_factor=None, original_max_position_embeddings=None, reuse_fp32_param=False, recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_in_advance=False, square_alibi_mask=False, fill_neg_inf=False, no_shared_storage=True, enable_high_availability=False, enable_optimizer_state_local_copy=False, enable_hbmfault_repair=False, context_parallel_algo='ulysses_cp_algo', ulysses_degree_in_cp=None, cp_attention_mask_type='causal', use_cp_send_recv_overlap=False, cp_window_size=1, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, kv_head_repeat_before_uly_alltoall=True, multi_head_latent_attention=False, q_lora_rank=None, kv_lora_rank=None, v_head_dim=None, qk_rope_head_dim=None, qk_nope_head_dim=None, rope_scaling_beta_fast=32, rope_scaling_beta_slow=1, rope_scaling_factor=1.0, rope_scaling_mscale=1.0, rope_scaling_mscale_all_dim=0.0, rope_scaling_original_max_position_embeddings=None, moe_intermediate_size=None, n_shared_experts=None, topk_group=None, routed_scaling_factor=None, norm_topk_prob=False, seq_aux=False, first_k_dense_replace=None, moe_layer_freq=None, moe_device_level_aux_loss_coeff=0.0, moe_comm_aux_loss_coeff=0.0, dpo_beta=0.1, dpo_loss_type='sigmoid', dpo_ftx=0.0, ref_model=None, dpo_label_smoothing=0.0, pref_ftx=0.0, is_pairwise_dataset=False, rank=5, world_size=8, log_name='gpt_1_3B_5stages_2024-12-6-16-11-19', num_stages=5, num_gpus=[1, 1, 2, 2, 2], resharding_stages=[False, False, False, False, False], num_ops_in_each_stage=[10, 9, 18, 18, 18], tensor_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], data_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], recompute_ops=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], algo_of_each_op=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main

world_size: 8
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 60, in validate_json_args
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 60, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:06:54 (PID:241637, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
[ERROR] 2024-12-06-20:06:54 (PID:241639, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp_rew0nhq'>
  _warnings.warn(warn_message, ResourceWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpgpbxjq28'>
  _warnings.warn(warn_message, ResourceWarning)
Namespace(num_layers=24, encoder_num_layers=None, decoder_num_layers=None, hidden_size=2048, ffn_hidden_size=None, num_attention_heads=32, kv_channels=None, group_query_attention=False, num_query_groups=1, max_position_embeddings=2048, position_embedding_type='learned_absolute', use_rotary_position_embeddings=False, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='LayerNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=None, end_weight_decay=None, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=2, batch_size=None, global_batch_size=1024, rampup_batch_size=None, recompute_activations=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=None, empty_unused_memory_level=0, checkpoint_activations=[False, False, False, False, False], train_iters=3, train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, use_flash_attn=True, add_bias_linear=True, optimizer='adam', dataloader_type=None, async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, use_mcore_models=True, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=320000, lr_decay_samples=None, lr_warmup_fraction=0.001, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, warmup=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save=None, save_interval=None, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save=False, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=5, pipeline_model_parallel_split_rank=None, model_parallel_size=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', overlap_grad_reduce=False, delay_grad_reduce=True, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=True, context_parallel_size=1, nccl_communicator_config_path=None, eval_iters=0, eval_interval=1000, test_mode=False, skip_train=False, data_path=None, split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=None, vocab_file='vocabs_file/gpt2-vocab.json', merge_file='vocabs_file/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=None, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_grouped_gemm=False, moe_input_jitter_eps=None, moe_token_dropping=False, moe_per_layer_logging=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2e-tracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, yaml_cfg=None, flexpipe=True, flexpipe_config='/workspace/RC4/ModelLink/aceso_execute/logs/configs/gpt/1_3B/top_configs/gpt_1_3B_5stages_2024-12-6-16-11-19.json', interleave_factor=1, checkpoint_stages=[], log_path='/workspace/RC4/ModelLink/aceso_execute/logs/runtime/gpt/1_3B/', prof_op=False, prof_tp_size=None, prof_path=None, prof_cache_file=None, prof_model_name='all', prof_model_size='all', prof_time_only=False, prof_memory_only=False, prof_warmup_times=20, prof_repeat_times=[50], prof_warmup_threshold=None, prof_repeat_threshold=None, prof_skip_running=False, prof_num_nodes=None, prof_node_rank=None, prof_ref_data=None, prof_mbs_list=None, use_fused_rmsnorm=False, use_fused_swiglu=False, use_fused_rotary_pos_emb=False, use_fused_ring_attention_update=False, use_mc2=False, padded_vocab_size=None, embed_layernorm=False, use_glm_rope=False, sliding_window=None, output_layer_slice_num=1, lora_target_modules=[], lora_load=None, lora_r=16, lora_alpha=32, lora_modules_to_save=None, lora_register_forward_hook=['word_embeddings', 'input_layernorm'], lora_fusion=False, is_instruction_dataset=False, full_shuffle_instruction_dataset=False, variable_seq_lengths=False, tokenizer_kwargs=None, tokenizer_padding_side='right', tokenizer_type='GPT2BPETokenizer', tokenizer_name_or_path=None, tokenizer_not_use_fast=True, input_layernorm_in_fp32=False, no_shuffle=False, moe_router_topk=2, moe_router_load_balancing_type='aux_loss', expert_interval=1, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=0.0, moe_train_capacity_factor=1.0, use_fused_moe_token_permute_and_unpermute=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_token_dispatcher_type='allgather', noisy_gate_policy=None, enable_token_rearrange_opt=False, embedding_multiplier_scale=1.0, input_jitter=True, post_norm=False, output_multiplier_scale=None, moe_permutation_async_comm=False, shared_expert_gate=False, shared_expert_gate_output_dimension=1, num_layer_list=None, profile_ranks=[-1], profile_level='level0', profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_with_cpu=False, profile_save_path='./profile_dir', add_qkv_bias=False, add_dense_bias=False, skip_bias_add=True, add_rmsnorm_offset=False, geglu=False, input_embeds_norm=False, gelu_tanh=False, output_logit_softcapping=None, attn_logit_softcapping=None, query_pre_attn_scalar=None, interleave_sliding_window=None, stage=None, gradient_accumulation_fusion=False, transformer_impl='local', enable_recompute_layers_per_pp_rank=False, pre_tockens=65536, next_tockens=0, sparse_mode=0, shape_order='SBH', use_deter_comp=False, jit_compile=False, prompt_type=None, pad_to_multiple_of=8, scale_emb=None, dim_model_base=None, no_cut_token=False, scale_depth=None, swap_attention=False, swap_modules=None, load_checkpoint_loosely=False, no_post_layer_norm=False, distributed_timeout_minutes=45, rotary_base=None, rope_scaling_type=None, low_freq_factor=None, high_freq_factor=None, original_max_position_embeddings=None, reuse_fp32_param=False, recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_in_advance=False, square_alibi_mask=False, fill_neg_inf=False, no_shared_storage=True, enable_high_availability=False, enable_optimizer_state_local_copy=False, enable_hbmfault_repair=False, context_parallel_algo='ulysses_cp_algo', ulysses_degree_in_cp=None, cp_attention_mask_type='causal', use_cp_send_recv_overlap=False, cp_window_size=1, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, kv_head_repeat_before_uly_alltoall=True, multi_head_latent_attention=False, q_lora_rank=None, kv_lora_rank=None, v_head_dim=None, qk_rope_head_dim=None, qk_nope_head_dim=None, rope_scaling_beta_fast=32, rope_scaling_beta_slow=1, rope_scaling_factor=1.0, rope_scaling_mscale=1.0, rope_scaling_mscale_all_dim=0.0, rope_scaling_original_max_position_embeddings=None, moe_intermediate_size=None, n_shared_experts=None, topk_group=None, routed_scaling_factor=None, norm_topk_prob=False, seq_aux=False, first_k_dense_replace=None, moe_layer_freq=None, moe_device_level_aux_loss_coeff=0.0, moe_comm_aux_loss_coeff=0.0, dpo_beta=0.1, dpo_loss_type='sigmoid', dpo_ftx=0.0, ref_model=None, dpo_label_smoothing=0.0, pref_ftx=0.0, is_pairwise_dataset=False, rank=4, world_size=8, log_name='gpt_1_3B_5stages_2024-12-6-16-11-19', num_stages=5, num_gpus=[1, 1, 2, 2, 2], resharding_stages=[False, False, False, False, False], num_ops_in_each_stage=[10, 9, 18, 18, 18], tensor_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], data_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], recompute_ops=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], algo_of_each_op=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 60, in validate_json_args
Namespace(num_layers=24, encoder_num_layers=None, decoder_num_layers=None, hidden_size=2048, ffn_hidden_size=None, num_attention_heads=32, kv_channels=None, group_query_attention=False, num_query_groups=1, max_position_embeddings=2048, position_embedding_type='learned_absolute', use_rotary_position_embeddings=False, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='LayerNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=None, end_weight_decay=None, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=2, batch_size=None, global_batch_size=1024, rampup_batch_size=None, recompute_activations=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=None, empty_unused_memory_level=0, checkpoint_activations=[False, False, False, False, False], train_iters=3, train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, use_flash_attn=True, add_bias_linear=True, optimizer='adam', dataloader_type=None, async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, use_mcore_models=True, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=320000, lr_decay_samples=None, lr_warmup_fraction=0.001, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, warmup=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save=None, save_interval=None, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save=False, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=5, pipeline_model_parallel_split_rank=None, model_parallel_size=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', overlap_grad_reduce=False, delay_grad_reduce=True, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=True, context_parallel_size=1, nccl_communicator_config_path=None, eval_iters=0, eval_interval=1000, test_mode=False, skip_train=False, data_path=None, split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=None, vocab_file='vocabs_file/gpt2-vocab.json', merge_file='vocabs_file/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=None, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_grouped_gemm=False, moe_input_jitter_eps=None, moe_token_dropping=False, moe_per_layer_logging=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2e-tracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, yaml_cfg=None, flexpipe=True, flexpipe_config='/workspace/RC4/ModelLink/aceso_execute/logs/configs/gpt/1_3B/top_configs/gpt_1_3B_5stages_2024-12-6-16-11-19.json', interleave_factor=1, checkpoint_stages=[], log_path='/workspace/RC4/ModelLink/aceso_execute/logs/runtime/gpt/1_3B/', prof_op=False, prof_tp_size=None, prof_path=None, prof_cache_file=None, prof_model_name='all', prof_model_size='all', prof_time_only=False, prof_memory_only=False, prof_warmup_times=20, prof_repeat_times=[50], prof_warmup_threshold=None, prof_repeat_threshold=None, prof_skip_running=False, prof_num_nodes=None, prof_node_rank=None, prof_ref_data=None, prof_mbs_list=None, use_fused_rmsnorm=False, use_fused_swiglu=False, use_fused_rotary_pos_emb=False, use_fused_ring_attention_update=False, use_mc2=False, padded_vocab_size=None, embed_layernorm=False, use_glm_rope=False, sliding_window=None, output_layer_slice_num=1, lora_target_modules=[], lora_load=None, lora_r=16, lora_alpha=32, lora_modules_to_save=None, lora_register_forward_hook=['word_embeddings', 'input_layernorm'], lora_fusion=False, is_instruction_dataset=False, full_shuffle_instruction_dataset=False, variable_seq_lengths=False, tokenizer_kwargs=None, tokenizer_padding_side='right', tokenizer_type='GPT2BPETokenizer', tokenizer_name_or_path=None, tokenizer_not_use_fast=True, input_layernorm_in_fp32=False, no_shuffle=False, moe_router_topk=2, moe_router_load_balancing_type='aux_loss', expert_interval=1, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=0.0, moe_train_capacity_factor=1.0, use_fused_moe_token_permute_and_unpermute=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_token_dispatcher_type='allgather', noisy_gate_policy=None, enable_token_rearrange_opt=False, embedding_multiplier_scale=1.0, input_jitter=True, post_norm=False, output_multiplier_scale=None, moe_permutation_async_comm=False, shared_expert_gate=False, shared_expert_gate_output_dimension=1, num_layer_list=None, profile_ranks=[-1], profile_level='level0', profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_with_cpu=False, profile_save_path='./profile_dir', add_qkv_bias=False, add_dense_bias=False, skip_bias_add=True, add_rmsnorm_offset=False, geglu=False, input_embeds_norm=False, gelu_tanh=False, output_logit_softcapping=None, attn_logit_softcapping=None, query_pre_attn_scalar=None, interleave_sliding_window=None, stage=None, gradient_accumulation_fusion=False, transformer_impl='local', enable_recompute_layers_per_pp_rank=False, pre_tockens=65536, next_tockens=0, sparse_mode=0, shape_order='SBH', use_deter_comp=False, jit_compile=False, prompt_type=None, pad_to_multiple_of=8, scale_emb=None, dim_model_base=None, no_cut_token=False, scale_depth=None, swap_attention=False, swap_modules=None, load_checkpoint_loosely=False, no_post_layer_norm=False, distributed_timeout_minutes=45, rotary_base=None, rope_scaling_type=None, low_freq_factor=None, high_freq_factor=None, original_max_position_embeddings=None, reuse_fp32_param=False, recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_in_advance=False, square_alibi_mask=False, fill_neg_inf=False, no_shared_storage=True, enable_high_availability=False, enable_optimizer_state_local_copy=False, enable_hbmfault_repair=False, context_parallel_algo='ulysses_cp_algo', ulysses_degree_in_cp=None, cp_attention_mask_type='causal', use_cp_send_recv_overlap=False, cp_window_size=1, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, kv_head_repeat_before_uly_alltoall=True, multi_head_latent_attention=False, q_lora_rank=None, kv_lora_rank=None, v_head_dim=None, qk_rope_head_dim=None, qk_nope_head_dim=None, rope_scaling_beta_fast=32, rope_scaling_beta_slow=1, rope_scaling_factor=1.0, rope_scaling_mscale=1.0, rope_scaling_mscale_all_dim=0.0, rope_scaling_original_max_position_embeddings=None, moe_intermediate_size=None, n_shared_experts=None, topk_group=None, routed_scaling_factor=None, norm_topk_prob=False, seq_aux=False, first_k_dense_replace=None, moe_layer_freq=None, moe_device_level_aux_loss_coeff=0.0, moe_comm_aux_loss_coeff=0.0, dpo_beta=0.1, dpo_loss_type='sigmoid', dpo_ftx=0.0, ref_model=None, dpo_label_smoothing=0.0, pref_ftx=0.0, is_pairwise_dataset=False, rank=2, world_size=8, log_name='gpt_1_3B_5stages_2024-12-6-16-11-19', num_stages=5, num_gpus=[1, 1, 2, 2, 2], resharding_stages=[False, False, False, False, False], num_ops_in_each_stage=[10, 9, 18, 18, 18], tensor_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], data_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], recompute_ops=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], algo_of_each_op=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"

AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
[ERROR] 2024-12-06-20:06:54 (PID:241638, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 60, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpl1jki4vs'>
  _warnings.warn(warn_message, ResourceWarning)
[ERROR] 2024-12-06-20:06:55 (PID:241636, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp6siw79kq'>
  _warnings.warn(warn_message, ResourceWarning)
Namespace(num_layers=24, encoder_num_layers=None, decoder_num_layers=None, hidden_size=2048, ffn_hidden_size=None, num_attention_heads=32, kv_channels=None, group_query_attention=False, num_query_groups=1, max_position_embeddings=2048, position_embedding_type='learned_absolute', use_rotary_position_embeddings=False, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='LayerNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=None, end_weight_decay=None, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=2, batch_size=None, global_batch_size=1024, rampup_batch_size=None, recompute_activations=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=None, empty_unused_memory_level=0, checkpoint_activations=[False, False, False, False, False], train_iters=3, train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, use_flash_attn=True, add_bias_linear=True, optimizer='adam', dataloader_type=None, async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, use_mcore_models=True, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=320000, lr_decay_samples=None, lr_warmup_fraction=0.001, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, warmup=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save=None, save_interval=None, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save=False, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=5, pipeline_model_parallel_split_rank=None, model_parallel_size=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', overlap_grad_reduce=False, delay_grad_reduce=True, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=True, context_parallel_size=1, nccl_communicator_config_path=None, eval_iters=0, eval_interval=1000, test_mode=False, skip_train=False, data_path=None, split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=None, vocab_file='vocabs_file/gpt2-vocab.json', merge_file='vocabs_file/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=None, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_grouped_gemm=False, moe_input_jitter_eps=None, moe_token_dropping=False, moe_per_layer_logging=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2e-tracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, yaml_cfg=None, flexpipe=True, flexpipe_config='/workspace/RC4/ModelLink/aceso_execute/logs/configs/gpt/1_3B/top_configs/gpt_1_3B_5stages_2024-12-6-16-11-19.json', interleave_factor=1, checkpoint_stages=[], log_path='/workspace/RC4/ModelLink/aceso_execute/logs/runtime/gpt/1_3B/', prof_op=False, prof_tp_size=None, prof_path=None, prof_cache_file=None, prof_model_name='all', prof_model_size='all', prof_time_only=False, prof_memory_only=False, prof_warmup_times=20, prof_repeat_times=[50], prof_warmup_threshold=None, prof_repeat_threshold=None, prof_skip_running=False, prof_num_nodes=None, prof_node_rank=None, prof_ref_data=None, prof_mbs_list=None, use_fused_rmsnorm=False, use_fused_swiglu=False, use_fused_rotary_pos_emb=False, use_fused_ring_attention_update=False, use_mc2=False, padded_vocab_size=None, embed_layernorm=False, use_glm_rope=False, sliding_window=None, output_layer_slice_num=1, lora_target_modules=[], lora_load=None, lora_r=16, lora_alpha=32, lora_modules_to_save=None, lora_register_forward_hook=['word_embeddings', 'input_layernorm'], lora_fusion=False, is_instruction_dataset=False, full_shuffle_instruction_dataset=False, variable_seq_lengths=False, tokenizer_kwargs=None, tokenizer_padding_side='right', tokenizer_type='GPT2BPETokenizer', tokenizer_name_or_path=None, tokenizer_not_use_fast=True, input_layernorm_in_fp32=False, no_shuffle=False, moe_router_topk=2, moe_router_load_balancing_type='aux_loss', expert_interval=1, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=0.0, moe_train_capacity_factor=1.0, use_fused_moe_token_permute_and_unpermute=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_token_dispatcher_type='allgather', noisy_gate_policy=None, enable_token_rearrange_opt=False, embedding_multiplier_scale=1.0, input_jitter=True, post_norm=False, output_multiplier_scale=None, moe_permutation_async_comm=False, shared_expert_gate=False, shared_expert_gate_output_dimension=1, num_layer_list=None, profile_ranks=[-1], profile_level='level0', profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_with_cpu=False, profile_save_path='./profile_dir', add_qkv_bias=False, add_dense_bias=False, skip_bias_add=True, add_rmsnorm_offset=False, geglu=False, input_embeds_norm=False, gelu_tanh=False, output_logit_softcapping=None, attn_logit_softcapping=None, query_pre_attn_scalar=None, interleave_sliding_window=None, stage=None, gradient_accumulation_fusion=False, transformer_impl='local', enable_recompute_layers_per_pp_rank=False, pre_tockens=65536, next_tockens=0, sparse_mode=0, shape_order='SBH', use_deter_comp=False, jit_compile=False, prompt_type=None, pad_to_multiple_of=8, scale_emb=None, dim_model_base=None, no_cut_token=False, scale_depth=None, swap_attention=False, swap_modules=None, load_checkpoint_loosely=False, no_post_layer_norm=False, distributed_timeout_minutes=45, rotary_base=None, rope_scaling_type=None, low_freq_factor=None, high_freq_factor=None, original_max_position_embeddings=None, reuse_fp32_param=False, recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_in_advance=False, square_alibi_mask=False, fill_neg_inf=False, no_shared_storage=True, enable_high_availability=False, enable_optimizer_state_local_copy=False, enable_hbmfault_repair=False, context_parallel_algo='ulysses_cp_algo', ulysses_degree_in_cp=None, cp_attention_mask_type='causal', use_cp_send_recv_overlap=False, cp_window_size=1, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, kv_head_repeat_before_uly_alltoall=True, multi_head_latent_attention=False, q_lora_rank=None, kv_lora_rank=None, v_head_dim=None, qk_rope_head_dim=None, qk_nope_head_dim=None, rope_scaling_beta_fast=32, rope_scaling_beta_slow=1, rope_scaling_factor=1.0, rope_scaling_mscale=1.0, rope_scaling_mscale_all_dim=0.0, rope_scaling_original_max_position_embeddings=None, moe_intermediate_size=None, n_shared_experts=None, topk_group=None, routed_scaling_factor=None, norm_topk_prob=False, seq_aux=False, first_k_dense_replace=None, moe_layer_freq=None, moe_device_level_aux_loss_coeff=0.0, moe_comm_aux_loss_coeff=0.0, dpo_beta=0.1, dpo_loss_type='sigmoid', dpo_ftx=0.0, ref_model=None, dpo_label_smoothing=0.0, pref_ftx=0.0, is_pairwise_dataset=False, rank=1, world_size=8, log_name='gpt_1_3B_5stages_2024-12-6-16-11-19', num_stages=5, num_gpus=[1, 1, 2, 2, 2], resharding_stages=[False, False, False, False, False], num_ops_in_each_stage=[10, 9, 18, 18, 18], tensor_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], data_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], recompute_ops=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], algo_of_each_op=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 60, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:06:55 (PID:241635, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp1wfvlfyc'>
  _warnings.warn(warn_message, ResourceWarning)
Namespace(num_layers=24, encoder_num_layers=None, decoder_num_layers=None, hidden_size=2048, ffn_hidden_size=None, num_attention_heads=32, kv_channels=None, group_query_attention=False, num_query_groups=1, max_position_embeddings=2048, position_embedding_type='learned_absolute', use_rotary_position_embeddings=False, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='LayerNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=None, end_weight_decay=None, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=2, batch_size=None, global_batch_size=1024, rampup_batch_size=None, recompute_activations=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=None, empty_unused_memory_level=0, checkpoint_activations=[False, False, False, False, False], train_iters=3, train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, use_flash_attn=True, add_bias_linear=True, optimizer='adam', dataloader_type=None, async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, use_mcore_models=True, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=320000, lr_decay_samples=None, lr_warmup_fraction=0.001, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, warmup=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save=None, save_interval=None, no_save_optim=None, no_save_rng=None, load=None, no_load_optim=None, no_load_rng=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save=False, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=5, pipeline_model_parallel_split_rank=None, model_parallel_size=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', overlap_grad_reduce=False, delay_grad_reduce=True, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=True, context_parallel_size=1, nccl_communicator_config_path=None, eval_iters=0, eval_interval=1000, test_mode=False, skip_train=False, data_path=None, split='949,50,1', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=None, vocab_file='vocabs_file/gpt2-vocab.json', merge_file='vocabs_file/gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=None, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_model=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_grouped_gemm=False, moe_input_jitter_eps=None, moe_token_dropping=False, moe_per_layer_logging=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', enable_one_logger=False, one_logger_project='e2e-tracking', one_logger_entity='hwinf_dcm', one_logger_run_name=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, yaml_cfg=None, flexpipe=True, flexpipe_config='/workspace/RC4/ModelLink/aceso_execute/logs/configs/gpt/1_3B/top_configs/gpt_1_3B_5stages_2024-12-6-16-11-19.json', interleave_factor=1, checkpoint_stages=[], log_path='/workspace/RC4/ModelLink/aceso_execute/logs/runtime/gpt/1_3B/', prof_op=False, prof_tp_size=None, prof_path=None, prof_cache_file=None, prof_model_name='all', prof_model_size='all', prof_time_only=False, prof_memory_only=False, prof_warmup_times=20, prof_repeat_times=[50], prof_warmup_threshold=None, prof_repeat_threshold=None, prof_skip_running=False, prof_num_nodes=None, prof_node_rank=None, prof_ref_data=None, prof_mbs_list=None, use_fused_rmsnorm=False, use_fused_swiglu=False, use_fused_rotary_pos_emb=False, use_fused_ring_attention_update=False, use_mc2=False, padded_vocab_size=None, embed_layernorm=False, use_glm_rope=False, sliding_window=None, output_layer_slice_num=1, lora_target_modules=[], lora_load=None, lora_r=16, lora_alpha=32, lora_modules_to_save=None, lora_register_forward_hook=['word_embeddings', 'input_layernorm'], lora_fusion=False, is_instruction_dataset=False, full_shuffle_instruction_dataset=False, variable_seq_lengths=False, tokenizer_kwargs=None, tokenizer_padding_side='right', tokenizer_type='GPT2BPETokenizer', tokenizer_name_or_path=None, tokenizer_not_use_fast=True, input_layernorm_in_fp32=False, no_shuffle=False, moe_router_topk=2, moe_router_load_balancing_type='aux_loss', expert_interval=1, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=0.0, moe_train_capacity_factor=1.0, use_fused_moe_token_permute_and_unpermute=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_token_dispatcher_type='allgather', noisy_gate_policy=None, enable_token_rearrange_opt=False, embedding_multiplier_scale=1.0, input_jitter=True, post_norm=False, output_multiplier_scale=None, moe_permutation_async_comm=False, shared_expert_gate=False, shared_expert_gate_output_dimension=1, num_layer_list=None, profile_ranks=[-1], profile_level='level0', profile_with_stack=False, profile_with_memory=False, profile_record_shapes=False, profile_with_cpu=False, profile_save_path='./profile_dir', add_qkv_bias=False, add_dense_bias=False, skip_bias_add=True, add_rmsnorm_offset=False, geglu=False, input_embeds_norm=False, gelu_tanh=False, output_logit_softcapping=None, attn_logit_softcapping=None, query_pre_attn_scalar=None, interleave_sliding_window=None, stage=None, gradient_accumulation_fusion=False, transformer_impl='local', enable_recompute_layers_per_pp_rank=False, pre_tockens=65536, next_tockens=0, sparse_mode=0, shape_order='SBH', use_deter_comp=False, jit_compile=False, prompt_type=None, pad_to_multiple_of=8, scale_emb=None, dim_model_base=None, no_cut_token=False, scale_depth=None, swap_attention=False, swap_modules=None, load_checkpoint_loosely=False, no_post_layer_norm=False, distributed_timeout_minutes=45, rotary_base=None, rope_scaling_type=None, low_freq_factor=None, high_freq_factor=None, original_max_position_embeddings=None, reuse_fp32_param=False, recompute_activation_function=False, recompute_activation_function_num_layers=None, recompute_in_advance=False, square_alibi_mask=False, fill_neg_inf=False, no_shared_storage=True, enable_high_availability=False, enable_optimizer_state_local_copy=False, enable_hbmfault_repair=False, context_parallel_algo='ulysses_cp_algo', ulysses_degree_in_cp=None, cp_attention_mask_type='causal', use_cp_send_recv_overlap=False, cp_window_size=1, attention_mask_on_cpu=False, adaptive_cp_without_coarse=False, adaptive_cp_dynamic_attn_mask=False, adaptive_cp_only_reschedule=False, adaptive_cp_manually_set_mask_list=False, kv_head_repeat_before_uly_alltoall=True, multi_head_latent_attention=False, q_lora_rank=None, kv_lora_rank=None, v_head_dim=None, qk_rope_head_dim=None, qk_nope_head_dim=None, rope_scaling_beta_fast=32, rope_scaling_beta_slow=1, rope_scaling_factor=1.0, rope_scaling_mscale=1.0, rope_scaling_mscale_all_dim=0.0, rope_scaling_original_max_position_embeddings=None, moe_intermediate_size=None, n_shared_experts=None, topk_group=None, routed_scaling_factor=None, norm_topk_prob=False, seq_aux=False, first_k_dense_replace=None, moe_layer_freq=None, moe_device_level_aux_loss_coeff=0.0, moe_comm_aux_loss_coeff=0.0, dpo_beta=0.1, dpo_loss_type='sigmoid', dpo_ftx=0.0, ref_model=None, dpo_label_smoothing=0.0, pref_ftx=0.0, is_pairwise_dataset=False, rank=6, world_size=8, log_name='gpt_1_3B_5stages_2024-12-6-16-11-19', num_stages=5, num_gpus=[1, 1, 2, 2, 2], resharding_stages=[False, False, False, False, False], num_ops_in_each_stage=[10, 9, 18, 18, 18], tensor_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], data_parallel_size_of_each_op=[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], recompute_ops=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], algo_of_each_op=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 60, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:06:55 (PID:241640, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp8zj9m6hl'>
  _warnings.warn(warn_message, ResourceWarning)
[2024-12-06 20:07:01,015] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 241634) of binary: /home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9
