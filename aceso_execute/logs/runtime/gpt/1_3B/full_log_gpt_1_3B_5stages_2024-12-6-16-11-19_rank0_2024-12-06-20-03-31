[2024-12-06 20:03:34,046] torch.distributed.run: [WARNING] 
[2024-12-06 20:03:34,046] torch.distributed.run: [WARNING] *****************************************
[2024-12-06 20:03:34,046] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-06 20:03:34,046] torch.distributed.run: [WARNING] *****************************************
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:299: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/workspace/RC4/ModelLink/megatron/training/arguments.py:943: DeprecationWarning: invalid escape sequence \ 
  '   --rampup-batch-size 16 8 300000 \ '
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
ninja: no work to do.
Loading extension module adaptive_cp...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
ninja: no work to do.
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
world_size: 8
using world size: 8, context-parallel size: 1
--checkpoint-activations is no longer valid, use --recompute-activations, or, for more control, --recompute-granularity and --recompute-method.
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 59, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:03:42 (PID:241365, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpk0ngou16'>
  _warnings.warn(warn_message, ResourceWarning)
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 59, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:03:42 (PID:241371, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmplbued2wa'>
  _warnings.warn(warn_message, ResourceWarning)
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 59, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:03:42 (PID:241366, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp6f4arxkr'>
  _warnings.warn(warn_message, ResourceWarning)
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 59, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:03:42 (PID:241367, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpw82idpda'>
  _warnings.warn(warn_message, ResourceWarning)
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 59, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:03:43 (PID:241369, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpi7wckwqj'>
  _warnings.warn(warn_message, ResourceWarning)
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 59, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:03:43 (PID:241368, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp3owtltyj'>
  _warnings.warn(warn_message, ResourceWarning)
world_size: 8
Traceback (most recent call last):
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
world_size: 8
    main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
Traceback (most recent call last):
      File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 228, in <module>
pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
      File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
main()
  File "/workspace/RC4/ModelLink/pretrain_gpt.py", line 221, in main
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    pretrain(train_valid_test_datasets_provider,
  File "/workspace/RC4/ModelLink/modellink/training/training.py", line 290, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/workspace/RC4/ModelLink/modellink/training/initialize.py", line 93, in initialize_megatron
    validate_args(args, args_defaults)####
  File "/workspace/RC4/ModelLink/modellink/training/arguments.py", line 965, in wrapper
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 59, in validate_json_args
    megatron_validate_args(args, defaults)
  File "/workspace/RC4/ModelLink/megatron/training/arguments.py", line 533, in validate_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
    validate_json_args(args)
  File "/workspace/RC4/ModelLink/megatron/training/json_arguments.py", line 59, in validate_json_args
    assert num_ops_total == args.num_layers * 2 + 2, f"num_ops_in_each_stage should sum to num_layers * 2 + 2: {num_ops_total} {args.num_layers}"
AssertionError: num_ops_in_each_stage should sum to num_layers * 2 + 2: 73 24
[ERROR] 2024-12-06-20:03:43 (PID:241372, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
[ERROR] 2024-12-06-20:03:43 (PID:241370, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmprpre01qp'>
  _warnings.warn(warn_message, ResourceWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpn9d_vjl_'>
  _warnings.warn(warn_message, ResourceWarning)
[2024-12-06 20:03:44,140] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241366 closing signal SIGTERM
[2024-12-06 20:03:44,140] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241367 closing signal SIGTERM
[2024-12-06 20:03:44,141] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241368 closing signal SIGTERM
[2024-12-06 20:03:44,142] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241369 closing signal SIGTERM
[2024-12-06 20:03:44,142] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241370 closing signal SIGTERM
[2024-12-06 20:03:44,142] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241371 closing signal SIGTERM
[2024-12-06 20:03:44,142] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 241372 closing signal SIGTERM
[2024-12-06 20:03:45,685] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 241365) of binary: /home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9
Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-06_20:03:44
  host      : ascend01
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 241365)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
