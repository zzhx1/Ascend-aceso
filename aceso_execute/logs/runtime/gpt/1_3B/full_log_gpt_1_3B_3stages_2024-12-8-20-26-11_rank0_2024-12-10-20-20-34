[2024-12-10 20:20:37,156] torch.distributed.run: [WARNING] 
[2024-12-10 20:20:37,156] torch.distributed.run: [WARNING] *****************************************
[2024-12-10 20:20:37,156] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-10 20:20:37,156] torch.distributed.run: [WARNING] *****************************************
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:299: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
ninja: no work to do.
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,alibi,alibi}]
                       [--use-rotary-position-embeddings]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--no-position-embedding]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--use-flash-attn]
                       [--disable-bias-linear] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--use-mcore-models] [--manual-gc]
                       [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format {zarr,torch_dist}]
                       [--ckpt-fully-parallel-save] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--overlap-grad-reduce] [--no-delay-grad-reduce]
                       [--overlap-param-gather] [--delay-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS] [--moe-grouped-gemm]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dropping] [--moe-per-layer-logging]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR] [--enable-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-entity ONE_LOGGER_ENTITY]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--yaml-cfg YAML_CFG] [--flexpipe FLEXPIPE]
                       [--flexpipe-config FLEXPIPE_CONFIG]
                       [--interleave-factor INTERLEAVE_FACTOR]
                       [--checkpoint-stages CHECKPOINT_STAGES [CHECKPOINT_STAGES ...]]
                       [--log-path LOG_PATH] [--prof-op]
                       [--prof-tp-size PROF_TP_SIZE] [--prof-path PROF_PATH]
                       [--prof-cache-file PROF_CACHE_FILE]
                       [--prof-model-name PROF_MODEL_NAME]
                       [--prof-model-size PROF_MODEL_SIZE] [--prof-time-only]
                       [--prof-memory-only]
                       [--prof-warmup-times PROF_WARMUP_TIMES]
                       [--prof-repeat-times PROF_REPEAT_TIMES [PROF_REPEAT_TIMES ...]]
                       [--prof-warmup-threshold PROF_WARMUP_THRESHOLD]
                       [--prof-repeat-threshold PROF_REPEAT_THRESHOLD]
                       [--prof-skip-running] [--prof-num-nodes PROF_NUM_NODES]
                       [--prof-node-rank PROF_NODE_RANK]
                       [--prof-ref-data PROF_REF_DATA]
                       [--prof-mbs-list PROF_MBS_LIST [PROF_MBS_LIST ...]]
                       [--use-fused-rmsnorm] [--use-fused-swiglu]
                       [--use-fused-rotary-pos-emb]
                       [--use-fused-ring-attention-update] [--use-mc2]
                       [--padded-vocab-size PADDED_VOCAB_SIZE]
                       [--embed-layernorm] [--use-glm-rope]
                       [--sliding-window SLIDING_WINDOW]
                       [--output-layer-slice-num OUTPUT_LAYER_SLICE_NUM]
                       [--lora-target-modules LORA_TARGET_MODULES [LORA_TARGET_MODULES ...]]
                       [--lora-load LORA_LOAD] [--lora-r LORA_R]
                       [--lora-alpha LORA_ALPHA]
                       [--lora-modules-to-save LORA_MODULES_TO_SAVE [LORA_MODULES_TO_SAVE ...]]
                       [--lora-register-forward-hook LORA_REGISTER_FORWARD_HOOK [LORA_REGISTER_FORWARD_HOOK ...]]
                       [--lora-fusion] [--is-instruction-dataset]
                       [--full-shuffle-instruction-dataset]
                       [--variable-seq-lengths]
                       [--tokenizer-kwargs TOKENIZER_KWARGS [TOKENIZER_KWARGS ...]]
                       [--tokenizer-padding-side TOKENIZER_PADDING_SIDE]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,Llama2Tokenizer,PretrainedFromHF,NullTokenizer}]
                       [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                       [--tokenizer-not-use-fast] [--input-layernorm-in-fp32]
                       [--no-shuffle] [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-load-balancing-type {aux_loss,group_limited_greedy,softmax_topk,pai_megatron_aux_loss}]
                       [--expert-interval EXPERT_INTERVAL]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
                       [--use-fused-moe-token-permute-and-unpermute]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-token-dispatcher-type {allgather,alltoall}]
                       [--noisy-gate-policy NOISY_GATE_POLICY]
                       [--enable-token-rearrange-opt]
                       [--embedding-multiplier-scale EMBEDDING_MULTIPLIER_SCALE]
                       [--input-jitter] [--post-norm]
                       [--output-multiplier-scale OUTPUT_MULTIPLIER_SCALE]
                       [--moe-permutation-async-comm] [--shared-expert-gate]
                       [--shared-expert-gate-output-dimension SHARED_EXPERT_GATE_OUTPUT_DIMENSION]
                       [--num-layer-list NUM_LAYER_LIST]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--profile-level {level0,level1,level2}]
                       [--profile-with-stack] [--profile-with-memory]
                       [--profile-record-shapes] [--profile-with-cpu]
                       [--profile-save-path PROFILE_SAVE_PATH]
                       [--add-qkv-bias] [--add-dense-bias] [--skip-bias-add]
                       [--add-rmsnorm-offset] [--geglu] [--input-embeds-norm]
                       [--gelu-tanh]
                       [--output-logit-softcapping OUTPUT_LOGIT_SOFTCAPPING]
                       [--attn-logit-softcapping ATTN_LOGIT_SOFTCAPPING]
                       [--query-pre-attn-scalar QUERY_PRE_ATTN_SCALAR]
                       [--interleave-sliding-window INTERLEAVE_SLIDING_WINDOW]
                       [--stage {sft,dpo,rm}]
                       [--no-gradient-accumulation-fusion]
                       [--transformer-impl {local,transformer_engine}]
                       [--enable-recompute-layers-per-pp-rank]
                       [--pre-tockens PRE_TOCKENS]
                       [--next-tockens NEXT_TOCKENS]
                       [--sparse-mode SPARSE_MODE]
                       [--shape-order {SBH,BSH,BSND,BNSD}] [--use-deter-comp]
                       [--jit-compile]
                       [--prompt-type {default,empty,chatglm2,chatglm3,chatglm3_system,glm4,chatml,chatml_de,qwen,llama3,llama2,mistral,mixtral,gemma,alpaca,deepseek2,deepseek2-lite,cpm,baichuan2}]
                       [--pad-to-multiple-of PAD_TO_MULTIPLE_OF]
                       [--scale-emb SCALE_EMB]
                       [--dim-model-base DIM_MODEL_BASE] [--no-cut-token]
                       [--scale-depth SCALE_DEPTH] [--swap-attention]
                       [--swap-modules SWAP_MODULES]
                       [--load-checkpoint-loosely] [--no-post-layer-norm]
                       [--local-rank LOCAL_RANK]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--rotary-base ROTARY_BASE]
                       [--rope-scaling-type {llama3,yarn}]
                       [--low-freq-factor LOW_FREQ_FACTOR]
                       [--high-freq-factor HIGH_FREQ_FACTOR]
                       [--original-max-position-embeddings ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--reuse-fp32-param] [--recompute-activation-function]
                       [--recompute-activation-function-num-layers RECOMPUTE_ACTIVATION_FUNCTION_NUM_LAYERS]
                       [--recompute-in-advance] [--square-alibi-mask]
                       [--fill-neg-inf] [--no-shared-storage]
                       [--enable-high-availability]
                       [--enable-optimizer-state-local-copy]
                       [--enable-hbmfault-repair]
                       [--context-parallel-algo {ulysses_cp_algo,megatron_cp_algo,hybrid_cp_algo,adaptive_cp_algo,hybrid_adaptive_cp_algo}]
                       [--ulysses-degree-in-cp ULYSSES_DEGREE_IN_CP]
                       [--cp-attention-mask-type {causal,general}]
                       [--use-cp-send-recv-overlap]
                       [--cp-window-size CP_WINDOW_SIZE]
                       [--attention-mask-on-cpu]
                       [--adaptive-cp-without-coarse]
                       [--adaptive-cp-dynamic-attn-mask]
                       [--adaptive-cp-only-reschedule]
                       [--adaptive-cp-manually-set-mask-list]
                       [--kv-head-repeat-before-uly-alltoall]
                       [--multi-head-latent-attention]
                       [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK] [--v-head-dim V_HEAD_DIM]
                       [--qk-rope-head-dim QK_ROPE_HEAD_DIM]
                       [--qk-nope-head-dim QK_NOPE_HEAD_DIM]
                       [--rope-scaling-beta-fast ROPE_SCALING_BETA_FAST]
                       [--rope-scaling-beta-slow ROPE_SCALING_BETA_SLOW]
                       [--rope-scaling-factor ROPE_SCALING_FACTOR]
                       [--rope-scaling-mscale ROPE_SCALING_MSCALE]
                       [--rope-scaling-mscale-all-dim ROPE_SCALING_MSCALE_ALL_DIM]
                       [--rope-scaling-original-max-position-embeddings ROPE_SCALING_ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--moe-intermediate-size MOE_INTERMEDIATE_SIZE]
                       [--n-shared-experts N_SHARED_EXPERTS]
                       [--topk-group TOPK_GROUP]
                       [--routed-scaling-factor ROUTED_SCALING_FACTOR]
                       [--norm-topk-prob] [--seq-aux]
                       [--first-k-dense-replace FIRST_K_DENSE_REPLACE]
                       [--moe-layer-freq MOE_LAYER_FREQ]
                       [--moe-device-level-aux-loss-coeff MOE_DEVICE_LEVEL_AUX_LOSS_COEFF]
                       [--moe-comm-aux-loss-coeff MOE_COMM_AUX_LOSS_COEFF]
                       [--dpo-beta DPO_BETA]
                       [--dpo-loss-type {sigmoid,hinge,ipo}]
                       [--dpo-ftx DPO_FTX] [--ref-model REF_MODEL]
                       [--dpo-label-smoothing DPO_LABEL_SMOOTHING]
                       [--pref-ftx PREF_FTX] [--is-pairwise-dataset]
pretrain_gpt.py: error: unrecognized arguments: \ \ ### \ ### \ ## \ ## \ ## \ ## \ \ ##
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp_v26v3cl'>
  _warnings.warn(warn_message, ResourceWarning)
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,alibi,alibi}]
                       [--use-rotary-position-embeddings]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--no-position-embedding]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--use-flash-attn]
                       [--disable-bias-linear] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--use-mcore-models] [--manual-gc]
                       [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format {zarr,torch_dist}]
                       [--ckpt-fully-parallel-save] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--overlap-grad-reduce] [--no-delay-grad-reduce]
                       [--overlap-param-gather] [--delay-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS] [--moe-grouped-gemm]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dropping] [--moe-per-layer-logging]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR] [--enable-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-entity ONE_LOGGER_ENTITY]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--yaml-cfg YAML_CFG] [--flexpipe FLEXPIPE]
                       [--flexpipe-config FLEXPIPE_CONFIG]
                       [--interleave-factor INTERLEAVE_FACTOR]
                       [--checkpoint-stages CHECKPOINT_STAGES [CHECKPOINT_STAGES ...]]
                       [--log-path LOG_PATH] [--prof-op]
                       [--prof-tp-size PROF_TP_SIZE] [--prof-path PROF_PATH]
                       [--prof-cache-file PROF_CACHE_FILE]
                       [--prof-model-name PROF_MODEL_NAME]
                       [--prof-model-size PROF_MODEL_SIZE] [--prof-time-only]
                       [--prof-memory-only]
                       [--prof-warmup-times PROF_WARMUP_TIMES]
                       [--prof-repeat-times PROF_REPEAT_TIMES [PROF_REPEAT_TIMES ...]]
                       [--prof-warmup-threshold PROF_WARMUP_THRESHOLD]
                       [--prof-repeat-threshold PROF_REPEAT_THRESHOLD]
                       [--prof-skip-running] [--prof-num-nodes PROF_NUM_NODES]
                       [--prof-node-rank PROF_NODE_RANK]
                       [--prof-ref-data PROF_REF_DATA]
                       [--prof-mbs-list PROF_MBS_LIST [PROF_MBS_LIST ...]]
                       [--use-fused-rmsnorm] [--use-fused-swiglu]
                       [--use-fused-rotary-pos-emb]
                       [--use-fused-ring-attention-update] [--use-mc2]
                       [--padded-vocab-size PADDED_VOCAB_SIZE]
                       [--embed-layernorm] [--use-glm-rope]
                       [--sliding-window SLIDING_WINDOW]
                       [--output-layer-slice-num OUTPUT_LAYER_SLICE_NUM]
                       [--lora-target-modules LORA_TARGET_MODULES [LORA_TARGET_MODULES ...]]
                       [--lora-load LORA_LOAD] [--lora-r LORA_R]
                       [--lora-alpha LORA_ALPHA]
                       [--lora-modules-to-save LORA_MODULES_TO_SAVE [LORA_MODULES_TO_SAVE ...]]
                       [--lora-register-forward-hook LORA_REGISTER_FORWARD_HOOK [LORA_REGISTER_FORWARD_HOOK ...]]
                       [--lora-fusion] [--is-instruction-dataset]
                       [--full-shuffle-instruction-dataset]
                       [--variable-seq-lengths]
                       [--tokenizer-kwargs TOKENIZER_KWARGS [TOKENIZER_KWARGS ...]]
                       [--tokenizer-padding-side TOKENIZER_PADDING_SIDE]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,Llama2Tokenizer,PretrainedFromHF,NullTokenizer}]
                       [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                       [--tokenizer-not-use-fast] [--input-layernorm-in-fp32]
                       [--no-shuffle] [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-load-balancing-type {aux_loss,group_limited_greedy,softmax_topk,pai_megatron_aux_loss}]
                       [--expert-interval EXPERT_INTERVAL]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
                       [--use-fused-moe-token-permute-and-unpermute]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-token-dispatcher-type {allgather,alltoall}]
                       [--noisy-gate-policy NOISY_GATE_POLICY]
                       [--enable-token-rearrange-opt]
                       [--embedding-multiplier-scale EMBEDDING_MULTIPLIER_SCALE]
                       [--input-jitter] [--post-norm]
                       [--output-multiplier-scale OUTPUT_MULTIPLIER_SCALE]
                       [--moe-permutation-async-comm] [--shared-expert-gate]
                       [--shared-expert-gate-output-dimension SHARED_EXPERT_GATE_OUTPUT_DIMENSION]
                       [--num-layer-list NUM_LAYER_LIST]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--profile-level {level0,level1,level2}]
                       [--profile-with-stack] [--profile-with-memory]
                       [--profile-record-shapes] [--profile-with-cpu]
                       [--profile-save-path PROFILE_SAVE_PATH]
                       [--add-qkv-bias] [--add-dense-bias] [--skip-bias-add]
                       [--add-rmsnorm-offset] [--geglu] [--input-embeds-norm]
                       [--gelu-tanh]
                       [--output-logit-softcapping OUTPUT_LOGIT_SOFTCAPPING]
                       [--attn-logit-softcapping ATTN_LOGIT_SOFTCAPPING]
                       [--query-pre-attn-scalar QUERY_PRE_ATTN_SCALAR]
                       [--interleave-sliding-window INTERLEAVE_SLIDING_WINDOW]
                       [--stage {sft,dpo,rm}]
                       [--no-gradient-accumulation-fusion]
                       [--transformer-impl {local,transformer_engine}]
                       [--enable-recompute-layers-per-pp-rank]
                       [--pre-tockens PRE_TOCKENS]
                       [--next-tockens NEXT_TOCKENS]
                       [--sparse-mode SPARSE_MODE]
                       [--shape-order {SBH,BSH,BSND,BNSD}] [--use-deter-comp]
                       [--jit-compile]
                       [--prompt-type {default,empty,chatglm2,chatglm3,chatglm3_system,glm4,chatml,chatml_de,qwen,llama3,llama2,mistral,mixtral,gemma,alpaca,deepseek2,deepseek2-lite,cpm,baichuan2}]
                       [--pad-to-multiple-of PAD_TO_MULTIPLE_OF]
                       [--scale-emb SCALE_EMB]
                       [--dim-model-base DIM_MODEL_BASE] [--no-cut-token]
                       [--scale-depth SCALE_DEPTH] [--swap-attention]
                       [--swap-modules SWAP_MODULES]
                       [--load-checkpoint-loosely] [--no-post-layer-norm]
                       [--local-rank LOCAL_RANK]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--rotary-base ROTARY_BASE]
                       [--rope-scaling-type {llama3,yarn}]
                       [--low-freq-factor LOW_FREQ_FACTOR]
                       [--high-freq-factor HIGH_FREQ_FACTOR]
                       [--original-max-position-embeddings ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--reuse-fp32-param] [--recompute-activation-function]
                       [--recompute-activation-function-num-layers RECOMPUTE_ACTIVATION_FUNCTION_NUM_LAYERS]
                       [--recompute-in-advance] [--square-alibi-mask]
                       [--fill-neg-inf] [--no-shared-storage]
                       [--enable-high-availability]
                       [--enable-optimizer-state-local-copy]
                       [--enable-hbmfault-repair]
                       [--context-parallel-algo {ulysses_cp_algo,megatron_cp_algo,hybrid_cp_algo,adaptive_cp_algo,hybrid_adaptive_cp_algo}]
                       [--ulysses-degree-in-cp ULYSSES_DEGREE_IN_CP]
                       [--cp-attention-mask-type {causal,general}]
                       [--use-cp-send-recv-overlap]
                       [--cp-window-size CP_WINDOW_SIZE]
                       [--attention-mask-on-cpu]
                       [--adaptive-cp-without-coarse]
                       [--adaptive-cp-dynamic-attn-mask]
                       [--adaptive-cp-only-reschedule]
                       [--adaptive-cp-manually-set-mask-list]
                       [--kv-head-repeat-before-uly-alltoall]
                       [--multi-head-latent-attention]
                       [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK] [--v-head-dim V_HEAD_DIM]
                       [--qk-rope-head-dim QK_ROPE_HEAD_DIM]
                       [--qk-nope-head-dim QK_NOPE_HEAD_DIM]
                       [--rope-scaling-beta-fast ROPE_SCALING_BETA_FAST]
                       [--rope-scaling-beta-slow ROPE_SCALING_BETA_SLOW]
                       [--rope-scaling-factor ROPE_SCALING_FACTOR]
                       [--rope-scaling-mscale ROPE_SCALING_MSCALE]
                       [--rope-scaling-mscale-all-dim ROPE_SCALING_MSCALE_ALL_DIM]
                       [--rope-scaling-original-max-position-embeddings ROPE_SCALING_ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--moe-intermediate-size MOE_INTERMEDIATE_SIZE]
                       [--n-shared-experts N_SHARED_EXPERTS]
                       [--topk-group TOPK_GROUP]
                       [--routed-scaling-factor ROUTED_SCALING_FACTOR]
                       [--norm-topk-prob] [--seq-aux]
                       [--first-k-dense-replace FIRST_K_DENSE_REPLACE]
                       [--moe-layer-freq MOE_LAYER_FREQ]
                       [--moe-device-level-aux-loss-coeff MOE_DEVICE_LEVEL_AUX_LOSS_COEFF]
                       [--moe-comm-aux-loss-coeff MOE_COMM_AUX_LOSS_COEFF]
                       [--dpo-beta DPO_BETA]
                       [--dpo-loss-type {sigmoid,hinge,ipo}]
                       [--dpo-ftx DPO_FTX] [--ref-model REF_MODEL]
                       [--dpo-label-smoothing DPO_LABEL_SMOOTHING]
                       [--pref-ftx PREF_FTX] [--is-pairwise-dataset]
pretrain_gpt.py: error: unrecognized arguments: \ \ ### \ ### \ ## \ ## \ ## \ ## \ \ ##
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpr768qico'>
  _warnings.warn(warn_message, ResourceWarning)
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,alibi,alibi}]
                       [--use-rotary-position-embeddings]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--no-position-embedding]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--use-flash-attn]
                       [--disable-bias-linear] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--use-mcore-models] [--manual-gc]
                       [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format {zarr,torch_dist}]
                       [--ckpt-fully-parallel-save] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--overlap-grad-reduce] [--no-delay-grad-reduce]
                       [--overlap-param-gather] [--delay-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS] [--moe-grouped-gemm]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dropping] [--moe-per-layer-logging]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR] [--enable-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-entity ONE_LOGGER_ENTITY]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--yaml-cfg YAML_CFG] [--flexpipe FLEXPIPE]
                       [--flexpipe-config FLEXPIPE_CONFIG]
                       [--interleave-factor INTERLEAVE_FACTOR]
                       [--checkpoint-stages CHECKPOINT_STAGES [CHECKPOINT_STAGES ...]]
                       [--log-path LOG_PATH] [--prof-op]
                       [--prof-tp-size PROF_TP_SIZE] [--prof-path PROF_PATH]
                       [--prof-cache-file PROF_CACHE_FILE]
                       [--prof-model-name PROF_MODEL_NAME]
                       [--prof-model-size PROF_MODEL_SIZE] [--prof-time-only]
                       [--prof-memory-only]
                       [--prof-warmup-times PROF_WARMUP_TIMES]
                       [--prof-repeat-times PROF_REPEAT_TIMES [PROF_REPEAT_TIMES ...]]
                       [--prof-warmup-threshold PROF_WARMUP_THRESHOLD]
                       [--prof-repeat-threshold PROF_REPEAT_THRESHOLD]
                       [--prof-skip-running] [--prof-num-nodes PROF_NUM_NODES]
                       [--prof-node-rank PROF_NODE_RANK]
                       [--prof-ref-data PROF_REF_DATA]
                       [--prof-mbs-list PROF_MBS_LIST [PROF_MBS_LIST ...]]
                       [--use-fused-rmsnorm] [--use-fused-swiglu]
                       [--use-fused-rotary-pos-emb]
                       [--use-fused-ring-attention-update] [--use-mc2]
                       [--padded-vocab-size PADDED_VOCAB_SIZE]
                       [--embed-layernorm] [--use-glm-rope]
                       [--sliding-window SLIDING_WINDOW]
                       [--output-layer-slice-num OUTPUT_LAYER_SLICE_NUM]
                       [--lora-target-modules LORA_TARGET_MODULES [LORA_TARGET_MODULES ...]]
                       [--lora-load LORA_LOAD] [--lora-r LORA_R]
                       [--lora-alpha LORA_ALPHA]
                       [--lora-modules-to-save LORA_MODULES_TO_SAVE [LORA_MODULES_TO_SAVE ...]]
                       [--lora-register-forward-hook LORA_REGISTER_FORWARD_HOOK [LORA_REGISTER_FORWARD_HOOK ...]]
                       [--lora-fusion] [--is-instruction-dataset]
                       [--full-shuffle-instruction-dataset]
                       [--variable-seq-lengths]
                       [--tokenizer-kwargs TOKENIZER_KWARGS [TOKENIZER_KWARGS ...]]
                       [--tokenizer-padding-side TOKENIZER_PADDING_SIDE]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,Llama2Tokenizer,PretrainedFromHF,NullTokenizer}]
                       [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                       [--tokenizer-not-use-fast] [--input-layernorm-in-fp32]
                       [--no-shuffle] [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-load-balancing-type {aux_loss,group_limited_greedy,softmax_topk,pai_megatron_aux_loss}]
                       [--expert-interval EXPERT_INTERVAL]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
                       [--use-fused-moe-token-permute-and-unpermute]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-token-dispatcher-type {allgather,alltoall}]
                       [--noisy-gate-policy NOISY_GATE_POLICY]
                       [--enable-token-rearrange-opt]
                       [--embedding-multiplier-scale EMBEDDING_MULTIPLIER_SCALE]
                       [--input-jitter] [--post-norm]
                       [--output-multiplier-scale OUTPUT_MULTIPLIER_SCALE]
                       [--moe-permutation-async-comm] [--shared-expert-gate]
                       [--shared-expert-gate-output-dimension SHARED_EXPERT_GATE_OUTPUT_DIMENSION]
                       [--num-layer-list NUM_LAYER_LIST]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--profile-level {level0,level1,level2}]
                       [--profile-with-stack] [--profile-with-memory]
                       [--profile-record-shapes] [--profile-with-cpu]
                       [--profile-save-path PROFILE_SAVE_PATH]
                       [--add-qkv-bias] [--add-dense-bias] [--skip-bias-add]
                       [--add-rmsnorm-offset] [--geglu] [--input-embeds-norm]
                       [--gelu-tanh]
                       [--output-logit-softcapping OUTPUT_LOGIT_SOFTCAPPING]
                       [--attn-logit-softcapping ATTN_LOGIT_SOFTCAPPING]
                       [--query-pre-attn-scalar QUERY_PRE_ATTN_SCALAR]
                       [--interleave-sliding-window INTERLEAVE_SLIDING_WINDOW]
                       [--stage {sft,dpo,rm}]
                       [--no-gradient-accumulation-fusion]
                       [--transformer-impl {local,transformer_engine}]
                       [--enable-recompute-layers-per-pp-rank]
                       [--pre-tockens PRE_TOCKENS]
                       [--next-tockens NEXT_TOCKENS]
                       [--sparse-mode SPARSE_MODE]
                       [--shape-order {SBH,BSH,BSND,BNSD}] [--use-deter-comp]
                       [--jit-compile]
                       [--prompt-type {default,empty,chatglm2,chatglm3,chatglm3_system,glm4,chatml,chatml_de,qwen,llama3,llama2,mistral,mixtral,gemma,alpaca,deepseek2,deepseek2-lite,cpm,baichuan2}]
                       [--pad-to-multiple-of PAD_TO_MULTIPLE_OF]
                       [--scale-emb SCALE_EMB]
                       [--dim-model-base DIM_MODEL_BASE] [--no-cut-token]
                       [--scale-depth SCALE_DEPTH] [--swap-attention]
                       [--swap-modules SWAP_MODULES]
                       [--load-checkpoint-loosely] [--no-post-layer-norm]
                       [--local-rank LOCAL_RANK]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--rotary-base ROTARY_BASE]
                       [--rope-scaling-type {llama3,yarn}]
                       [--low-freq-factor LOW_FREQ_FACTOR]
                       [--high-freq-factor HIGH_FREQ_FACTOR]
                       [--original-max-position-embeddings ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--reuse-fp32-param] [--recompute-activation-function]
                       [--recompute-activation-function-num-layers RECOMPUTE_ACTIVATION_FUNCTION_NUM_LAYERS]
                       [--recompute-in-advance] [--square-alibi-mask]
                       [--fill-neg-inf] [--no-shared-storage]
                       [--enable-high-availability]
                       [--enable-optimizer-state-local-copy]
                       [--enable-hbmfault-repair]
                       [--context-parallel-algo {ulysses_cp_algo,megatron_cp_algo,hybrid_cp_algo,adaptive_cp_algo,hybrid_adaptive_cp_algo}]
                       [--ulysses-degree-in-cp ULYSSES_DEGREE_IN_CP]
                       [--cp-attention-mask-type {causal,general}]
                       [--use-cp-send-recv-overlap]
                       [--cp-window-size CP_WINDOW_SIZE]
                       [--attention-mask-on-cpu]
                       [--adaptive-cp-without-coarse]
                       [--adaptive-cp-dynamic-attn-mask]
                       [--adaptive-cp-only-reschedule]
                       [--adaptive-cp-manually-set-mask-list]
                       [--kv-head-repeat-before-uly-alltoall]
                       [--multi-head-latent-attention]
                       [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK] [--v-head-dim V_HEAD_DIM]
                       [--qk-rope-head-dim QK_ROPE_HEAD_DIM]
                       [--qk-nope-head-dim QK_NOPE_HEAD_DIM]
                       [--rope-scaling-beta-fast ROPE_SCALING_BETA_FAST]
                       [--rope-scaling-beta-slow ROPE_SCALING_BETA_SLOW]
                       [--rope-scaling-factor ROPE_SCALING_FACTOR]
                       [--rope-scaling-mscale ROPE_SCALING_MSCALE]
                       [--rope-scaling-mscale-all-dim ROPE_SCALING_MSCALE_ALL_DIM]
                       [--rope-scaling-original-max-position-embeddings ROPE_SCALING_ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--moe-intermediate-size MOE_INTERMEDIATE_SIZE]
                       [--n-shared-experts N_SHARED_EXPERTS]
                       [--topk-group TOPK_GROUP]
                       [--routed-scaling-factor ROUTED_SCALING_FACTOR]
                       [--norm-topk-prob] [--seq-aux]
                       [--first-k-dense-replace FIRST_K_DENSE_REPLACE]
                       [--moe-layer-freq MOE_LAYER_FREQ]
                       [--moe-device-level-aux-loss-coeff MOE_DEVICE_LEVEL_AUX_LOSS_COEFF]
                       [--moe-comm-aux-loss-coeff MOE_COMM_AUX_LOSS_COEFF]
                       [--dpo-beta DPO_BETA]
                       [--dpo-loss-type {sigmoid,hinge,ipo}]
                       [--dpo-ftx DPO_FTX] [--ref-model REF_MODEL]
                       [--dpo-label-smoothing DPO_LABEL_SMOOTHING]
                       [--pref-ftx PREF_FTX] [--is-pairwise-dataset]
pretrain_gpt.py: error: unrecognized arguments: \ \ ### \ ### \ ## \ ## \ ## \ ## \ \ ##
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpguocn14h'>
  _warnings.warn(warn_message, ResourceWarning)
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,alibi,alibi}]
                       [--use-rotary-position-embeddings]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--no-position-embedding]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--use-flash-attn]
                       [--disable-bias-linear] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--use-mcore-models] [--manual-gc]
                       [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format {zarr,torch_dist}]
                       [--ckpt-fully-parallel-save] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--overlap-grad-reduce] [--no-delay-grad-reduce]
                       [--overlap-param-gather] [--delay-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS] [--moe-grouped-gemm]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dropping] [--moe-per-layer-logging]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR] [--enable-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-entity ONE_LOGGER_ENTITY]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--yaml-cfg YAML_CFG] [--flexpipe FLEXPIPE]
                       [--flexpipe-config FLEXPIPE_CONFIG]
                       [--interleave-factor INTERLEAVE_FACTOR]
                       [--checkpoint-stages CHECKPOINT_STAGES [CHECKPOINT_STAGES ...]]
                       [--log-path LOG_PATH] [--prof-op]
                       [--prof-tp-size PROF_TP_SIZE] [--prof-path PROF_PATH]
                       [--prof-cache-file PROF_CACHE_FILE]
                       [--prof-model-name PROF_MODEL_NAME]
                       [--prof-model-size PROF_MODEL_SIZE] [--prof-time-only]
                       [--prof-memory-only]
                       [--prof-warmup-times PROF_WARMUP_TIMES]
                       [--prof-repeat-times PROF_REPEAT_TIMES [PROF_REPEAT_TIMES ...]]
                       [--prof-warmup-threshold PROF_WARMUP_THRESHOLD]
                       [--prof-repeat-threshold PROF_REPEAT_THRESHOLD]
                       [--prof-skip-running] [--prof-num-nodes PROF_NUM_NODES]
                       [--prof-node-rank PROF_NODE_RANK]
                       [--prof-ref-data PROF_REF_DATA]
                       [--prof-mbs-list PROF_MBS_LIST [PROF_MBS_LIST ...]]
                       [--use-fused-rmsnorm] [--use-fused-swiglu]
                       [--use-fused-rotary-pos-emb]
                       [--use-fused-ring-attention-update] [--use-mc2]
                       [--padded-vocab-size PADDED_VOCAB_SIZE]
                       [--embed-layernorm] [--use-glm-rope]
                       [--sliding-window SLIDING_WINDOW]
                       [--output-layer-slice-num OUTPUT_LAYER_SLICE_NUM]
                       [--lora-target-modules LORA_TARGET_MODULES [LORA_TARGET_MODULES ...]]
                       [--lora-load LORA_LOAD] [--lora-r LORA_R]
                       [--lora-alpha LORA_ALPHA]
                       [--lora-modules-to-save LORA_MODULES_TO_SAVE [LORA_MODULES_TO_SAVE ...]]
                       [--lora-register-forward-hook LORA_REGISTER_FORWARD_HOOK [LORA_REGISTER_FORWARD_HOOK ...]]
                       [--lora-fusion] [--is-instruction-dataset]
                       [--full-shuffle-instruction-dataset]
                       [--variable-seq-lengths]
                       [--tokenizer-kwargs TOKENIZER_KWARGS [TOKENIZER_KWARGS ...]]
                       [--tokenizer-padding-side TOKENIZER_PADDING_SIDE]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,Llama2Tokenizer,PretrainedFromHF,NullTokenizer}]
                       [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                       [--tokenizer-not-use-fast] [--input-layernorm-in-fp32]
                       [--no-shuffle] [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-load-balancing-type {aux_loss,group_limited_greedy,softmax_topk,pai_megatron_aux_loss}]
                       [--expert-interval EXPERT_INTERVAL]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
                       [--use-fused-moe-token-permute-and-unpermute]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-token-dispatcher-type {allgather,alltoall}]
                       [--noisy-gate-policy NOISY_GATE_POLICY]
                       [--enable-token-rearrange-opt]
                       [--embedding-multiplier-scale EMBEDDING_MULTIPLIER_SCALE]
                       [--input-jitter] [--post-norm]
                       [--output-multiplier-scale OUTPUT_MULTIPLIER_SCALE]
                       [--moe-permutation-async-comm] [--shared-expert-gate]
                       [--shared-expert-gate-output-dimension SHARED_EXPERT_GATE_OUTPUT_DIMENSION]
                       [--num-layer-list NUM_LAYER_LIST]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--profile-level {level0,level1,level2}]
                       [--profile-with-stack] [--profile-with-memory]
                       [--profile-record-shapes] [--profile-with-cpu]
                       [--profile-save-path PROFILE_SAVE_PATH]
                       [--add-qkv-bias] [--add-dense-bias] [--skip-bias-add]
                       [--add-rmsnorm-offset] [--geglu] [--input-embeds-norm]
                       [--gelu-tanh]
                       [--output-logit-softcapping OUTPUT_LOGIT_SOFTCAPPING]
                       [--attn-logit-softcapping ATTN_LOGIT_SOFTCAPPING]
                       [--query-pre-attn-scalar QUERY_PRE_ATTN_SCALAR]
                       [--interleave-sliding-window INTERLEAVE_SLIDING_WINDOW]
                       [--stage {sft,dpo,rm}]
                       [--no-gradient-accumulation-fusion]
                       [--transformer-impl {local,transformer_engine}]
                       [--enable-recompute-layers-per-pp-rank]
                       [--pre-tockens PRE_TOCKENS]
                       [--next-tockens NEXT_TOCKENS]
                       [--sparse-mode SPARSE_MODE]
                       [--shape-order {SBH,BSH,BSND,BNSD}] [--use-deter-comp]
                       [--jit-compile]
                       [--prompt-type {default,empty,chatglm2,chatglm3,chatglm3_system,glm4,chatml,chatml_de,qwen,llama3,llama2,mistral,mixtral,gemma,alpaca,deepseek2,deepseek2-lite,cpm,baichuan2}]
                       [--pad-to-multiple-of PAD_TO_MULTIPLE_OF]
                       [--scale-emb SCALE_EMB]
                       [--dim-model-base DIM_MODEL_BASE] [--no-cut-token]
                       [--scale-depth SCALE_DEPTH] [--swap-attention]
                       [--swap-modules SWAP_MODULES]
                       [--load-checkpoint-loosely] [--no-post-layer-norm]
                       [--local-rank LOCAL_RANK]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--rotary-base ROTARY_BASE]
                       [--rope-scaling-type {llama3,yarn}]
                       [--low-freq-factor LOW_FREQ_FACTOR]
                       [--high-freq-factor HIGH_FREQ_FACTOR]
                       [--original-max-position-embeddings ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--reuse-fp32-param] [--recompute-activation-function]
                       [--recompute-activation-function-num-layers RECOMPUTE_ACTIVATION_FUNCTION_NUM_LAYERS]
                       [--recompute-in-advance] [--square-alibi-mask]
                       [--fill-neg-inf] [--no-shared-storage]
                       [--enable-high-availability]
                       [--enable-optimizer-state-local-copy]
                       [--enable-hbmfault-repair]
                       [--context-parallel-algo {ulysses_cp_algo,megatron_cp_algo,hybrid_cp_algo,adaptive_cp_algo,hybrid_adaptive_cp_algo}]
                       [--ulysses-degree-in-cp ULYSSES_DEGREE_IN_CP]
                       [--cp-attention-mask-type {causal,general}]
                       [--use-cp-send-recv-overlap]
                       [--cp-window-size CP_WINDOW_SIZE]
                       [--attention-mask-on-cpu]
                       [--adaptive-cp-without-coarse]
                       [--adaptive-cp-dynamic-attn-mask]
                       [--adaptive-cp-only-reschedule]
                       [--adaptive-cp-manually-set-mask-list]
                       [--kv-head-repeat-before-uly-alltoall]
                       [--multi-head-latent-attention]
                       [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK] [--v-head-dim V_HEAD_DIM]
                       [--qk-rope-head-dim QK_ROPE_HEAD_DIM]
                       [--qk-nope-head-dim QK_NOPE_HEAD_DIM]
                       [--rope-scaling-beta-fast ROPE_SCALING_BETA_FAST]
                       [--rope-scaling-beta-slow ROPE_SCALING_BETA_SLOW]
                       [--rope-scaling-factor ROPE_SCALING_FACTOR]
                       [--rope-scaling-mscale ROPE_SCALING_MSCALE]
                       [--rope-scaling-mscale-all-dim ROPE_SCALING_MSCALE_ALL_DIM]
                       [--rope-scaling-original-max-position-embeddings ROPE_SCALING_ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--moe-intermediate-size MOE_INTERMEDIATE_SIZE]
                       [--n-shared-experts N_SHARED_EXPERTS]
                       [--topk-group TOPK_GROUP]
                       [--routed-scaling-factor ROUTED_SCALING_FACTOR]
                       [--norm-topk-prob] [--seq-aux]
                       [--first-k-dense-replace FIRST_K_DENSE_REPLACE]
                       [--moe-layer-freq MOE_LAYER_FREQ]
                       [--moe-device-level-aux-loss-coeff MOE_DEVICE_LEVEL_AUX_LOSS_COEFF]
                       [--moe-comm-aux-loss-coeff MOE_COMM_AUX_LOSS_COEFF]
                       [--dpo-beta DPO_BETA]
                       [--dpo-loss-type {sigmoid,hinge,ipo}]
                       [--dpo-ftx DPO_FTX] [--ref-model REF_MODEL]
                       [--dpo-label-smoothing DPO_LABEL_SMOOTHING]
                       [--pref-ftx PREF_FTX] [--is-pairwise-dataset]
pretrain_gpt.py: error: unrecognized arguments: \ \ ### \ ### \ ## \ ## \ ## \ ## \ \ ##
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp8u_imowy'>
  _warnings.warn(warn_message, ResourceWarning)
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,alibi,alibi}]
                       [--use-rotary-position-embeddings]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--no-position-embedding]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--use-flash-attn]
                       [--disable-bias-linear] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--use-mcore-models] [--manual-gc]
                       [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format {zarr,torch_dist}]
                       [--ckpt-fully-parallel-save] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--overlap-grad-reduce] [--no-delay-grad-reduce]
                       [--overlap-param-gather] [--delay-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS] [--moe-grouped-gemm]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dropping] [--moe-per-layer-logging]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR] [--enable-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-entity ONE_LOGGER_ENTITY]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--yaml-cfg YAML_CFG] [--flexpipe FLEXPIPE]
                       [--flexpipe-config FLEXPIPE_CONFIG]
                       [--interleave-factor INTERLEAVE_FACTOR]
                       [--checkpoint-stages CHECKPOINT_STAGES [CHECKPOINT_STAGES ...]]
                       [--log-path LOG_PATH] [--prof-op]
                       [--prof-tp-size PROF_TP_SIZE] [--prof-path PROF_PATH]
                       [--prof-cache-file PROF_CACHE_FILE]
                       [--prof-model-name PROF_MODEL_NAME]
                       [--prof-model-size PROF_MODEL_SIZE] [--prof-time-only]
                       [--prof-memory-only]
                       [--prof-warmup-times PROF_WARMUP_TIMES]
                       [--prof-repeat-times PROF_REPEAT_TIMES [PROF_REPEAT_TIMES ...]]
                       [--prof-warmup-threshold PROF_WARMUP_THRESHOLD]
                       [--prof-repeat-threshold PROF_REPEAT_THRESHOLD]
                       [--prof-skip-running] [--prof-num-nodes PROF_NUM_NODES]
                       [--prof-node-rank PROF_NODE_RANK]
                       [--prof-ref-data PROF_REF_DATA]
                       [--prof-mbs-list PROF_MBS_LIST [PROF_MBS_LIST ...]]
                       [--use-fused-rmsnorm] [--use-fused-swiglu]
                       [--use-fused-rotary-pos-emb]
                       [--use-fused-ring-attention-update] [--use-mc2]
                       [--padded-vocab-size PADDED_VOCAB_SIZE]
                       [--embed-layernorm] [--use-glm-rope]
                       [--sliding-window SLIDING_WINDOW]
                       [--output-layer-slice-num OUTPUT_LAYER_SLICE_NUM]
                       [--lora-target-modules LORA_TARGET_MODULES [LORA_TARGET_MODULES ...]]
                       [--lora-load LORA_LOAD] [--lora-r LORA_R]
                       [--lora-alpha LORA_ALPHA]
                       [--lora-modules-to-save LORA_MODULES_TO_SAVE [LORA_MODULES_TO_SAVE ...]]
                       [--lora-register-forward-hook LORA_REGISTER_FORWARD_HOOK [LORA_REGISTER_FORWARD_HOOK ...]]
                       [--lora-fusion] [--is-instruction-dataset]
                       [--full-shuffle-instruction-dataset]
                       [--variable-seq-lengths]
                       [--tokenizer-kwargs TOKENIZER_KWARGS [TOKENIZER_KWARGS ...]]
                       [--tokenizer-padding-side TOKENIZER_PADDING_SIDE]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,Llama2Tokenizer,PretrainedFromHF,NullTokenizer}]
                       [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                       [--tokenizer-not-use-fast] [--input-layernorm-in-fp32]
                       [--no-shuffle] [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-load-balancing-type {aux_loss,group_limited_greedy,softmax_topk,pai_megatron_aux_loss}]
                       [--expert-interval EXPERT_INTERVAL]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
                       [--use-fused-moe-token-permute-and-unpermute]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-token-dispatcher-type {allgather,alltoall}]
                       [--noisy-gate-policy NOISY_GATE_POLICY]
                       [--enable-token-rearrange-opt]
                       [--embedding-multiplier-scale EMBEDDING_MULTIPLIER_SCALE]
                       [--input-jitter] [--post-norm]
                       [--output-multiplier-scale OUTPUT_MULTIPLIER_SCALE]
                       [--moe-permutation-async-comm] [--shared-expert-gate]
                       [--shared-expert-gate-output-dimension SHARED_EXPERT_GATE_OUTPUT_DIMENSION]
                       [--num-layer-list NUM_LAYER_LIST]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--profile-level {level0,level1,level2}]
                       [--profile-with-stack] [--profile-with-memory]
                       [--profile-record-shapes] [--profile-with-cpu]
                       [--profile-save-path PROFILE_SAVE_PATH]
                       [--add-qkv-bias] [--add-dense-bias] [--skip-bias-add]
                       [--add-rmsnorm-offset] [--geglu] [--input-embeds-norm]
                       [--gelu-tanh]
                       [--output-logit-softcapping OUTPUT_LOGIT_SOFTCAPPING]
                       [--attn-logit-softcapping ATTN_LOGIT_SOFTCAPPING]
                       [--query-pre-attn-scalar QUERY_PRE_ATTN_SCALAR]
                       [--interleave-sliding-window INTERLEAVE_SLIDING_WINDOW]
                       [--stage {sft,dpo,rm}]
                       [--no-gradient-accumulation-fusion]
                       [--transformer-impl {local,transformer_engine}]
                       [--enable-recompute-layers-per-pp-rank]
                       [--pre-tockens PRE_TOCKENS]
                       [--next-tockens NEXT_TOCKENS]
                       [--sparse-mode SPARSE_MODE]
                       [--shape-order {SBH,BSH,BSND,BNSD}] [--use-deter-comp]
                       [--jit-compile]
                       [--prompt-type {default,empty,chatglm2,chatglm3,chatglm3_system,glm4,chatml,chatml_de,qwen,llama3,llama2,mistral,mixtral,gemma,alpaca,deepseek2,deepseek2-lite,cpm,baichuan2}]
                       [--pad-to-multiple-of PAD_TO_MULTIPLE_OF]
                       [--scale-emb SCALE_EMB]
                       [--dim-model-base DIM_MODEL_BASE] [--no-cut-token]
                       [--scale-depth SCALE_DEPTH] [--swap-attention]
                       [--swap-modules SWAP_MODULES]
                       [--load-checkpoint-loosely] [--no-post-layer-norm]
                       [--local-rank LOCAL_RANK]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--rotary-base ROTARY_BASE]
                       [--rope-scaling-type {llama3,yarn}]
                       [--low-freq-factor LOW_FREQ_FACTOR]
                       [--high-freq-factor HIGH_FREQ_FACTOR]
                       [--original-max-position-embeddings ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--reuse-fp32-param] [--recompute-activation-function]
                       [--recompute-activation-function-num-layers RECOMPUTE_ACTIVATION_FUNCTION_NUM_LAYERS]
                       [--recompute-in-advance] [--square-alibi-mask]
                       [--fill-neg-inf] [--no-shared-storage]
                       [--enable-high-availability]
                       [--enable-optimizer-state-local-copy]
                       [--enable-hbmfault-repair]
                       [--context-parallel-algo {ulysses_cp_algo,megatron_cp_algo,hybrid_cp_algo,adaptive_cp_algo,hybrid_adaptive_cp_algo}]
                       [--ulysses-degree-in-cp ULYSSES_DEGREE_IN_CP]
                       [--cp-attention-mask-type {causal,general}]
                       [--use-cp-send-recv-overlap]
                       [--cp-window-size CP_WINDOW_SIZE]
                       [--attention-mask-on-cpu]
                       [--adaptive-cp-without-coarse]
                       [--adaptive-cp-dynamic-attn-mask]
                       [--adaptive-cp-only-reschedule]
                       [--adaptive-cp-manually-set-mask-list]
                       [--kv-head-repeat-before-uly-alltoall]
                       [--multi-head-latent-attention]
                       [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK] [--v-head-dim V_HEAD_DIM]
                       [--qk-rope-head-dim QK_ROPE_HEAD_DIM]
                       [--qk-nope-head-dim QK_NOPE_HEAD_DIM]
                       [--rope-scaling-beta-fast ROPE_SCALING_BETA_FAST]
                       [--rope-scaling-beta-slow ROPE_SCALING_BETA_SLOW]
                       [--rope-scaling-factor ROPE_SCALING_FACTOR]
                       [--rope-scaling-mscale ROPE_SCALING_MSCALE]
                       [--rope-scaling-mscale-all-dim ROPE_SCALING_MSCALE_ALL_DIM]
                       [--rope-scaling-original-max-position-embeddings ROPE_SCALING_ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--moe-intermediate-size MOE_INTERMEDIATE_SIZE]
                       [--n-shared-experts N_SHARED_EXPERTS]
                       [--topk-group TOPK_GROUP]
                       [--routed-scaling-factor ROUTED_SCALING_FACTOR]
                       [--norm-topk-prob] [--seq-aux]
                       [--first-k-dense-replace FIRST_K_DENSE_REPLACE]
                       [--moe-layer-freq MOE_LAYER_FREQ]
                       [--moe-device-level-aux-loss-coeff MOE_DEVICE_LEVEL_AUX_LOSS_COEFF]
                       [--moe-comm-aux-loss-coeff MOE_COMM_AUX_LOSS_COEFF]
                       [--dpo-beta DPO_BETA]
                       [--dpo-loss-type {sigmoid,hinge,ipo}]
                       [--dpo-ftx DPO_FTX] [--ref-model REF_MODEL]
                       [--dpo-label-smoothing DPO_LABEL_SMOOTHING]
                       [--pref-ftx PREF_FTX] [--is-pairwise-dataset]
pretrain_gpt.py: error: unrecognized arguments: \ \ ### \ ### \ ## \ ## \ ## \ ## \ \ ##
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpm0j9i_mv'>
  _warnings.warn(warn_message, ResourceWarning)
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,alibi,alibi}]
                       [--use-rotary-position-embeddings]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--no-position-embedding]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--use-flash-attn]
                       [--disable-bias-linear] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--use-mcore-models] [--manual-gc]
                       [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format {zarr,torch_dist}]
                       [--ckpt-fully-parallel-save] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--overlap-grad-reduce] [--no-delay-grad-reduce]
                       [--overlap-param-gather] [--delay-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS] [--moe-grouped-gemm]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dropping] [--moe-per-layer-logging]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR] [--enable-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-entity ONE_LOGGER_ENTITY]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--yaml-cfg YAML_CFG] [--flexpipe FLEXPIPE]
                       [--flexpipe-config FLEXPIPE_CONFIG]
                       [--interleave-factor INTERLEAVE_FACTOR]
                       [--checkpoint-stages CHECKPOINT_STAGES [CHECKPOINT_STAGES ...]]
                       [--log-path LOG_PATH] [--prof-op]
                       [--prof-tp-size PROF_TP_SIZE] [--prof-path PROF_PATH]
                       [--prof-cache-file PROF_CACHE_FILE]
                       [--prof-model-name PROF_MODEL_NAME]
                       [--prof-model-size PROF_MODEL_SIZE] [--prof-time-only]
                       [--prof-memory-only]
                       [--prof-warmup-times PROF_WARMUP_TIMES]
                       [--prof-repeat-times PROF_REPEAT_TIMES [PROF_REPEAT_TIMES ...]]
                       [--prof-warmup-threshold PROF_WARMUP_THRESHOLD]
                       [--prof-repeat-threshold PROF_REPEAT_THRESHOLD]
                       [--prof-skip-running] [--prof-num-nodes PROF_NUM_NODES]
                       [--prof-node-rank PROF_NODE_RANK]
                       [--prof-ref-data PROF_REF_DATA]
                       [--prof-mbs-list PROF_MBS_LIST [PROF_MBS_LIST ...]]
                       [--use-fused-rmsnorm] [--use-fused-swiglu]
                       [--use-fused-rotary-pos-emb]
                       [--use-fused-ring-attention-update] [--use-mc2]
                       [--padded-vocab-size PADDED_VOCAB_SIZE]
                       [--embed-layernorm] [--use-glm-rope]
                       [--sliding-window SLIDING_WINDOW]
                       [--output-layer-slice-num OUTPUT_LAYER_SLICE_NUM]
                       [--lora-target-modules LORA_TARGET_MODULES [LORA_TARGET_MODULES ...]]
                       [--lora-load LORA_LOAD] [--lora-r LORA_R]
                       [--lora-alpha LORA_ALPHA]
                       [--lora-modules-to-save LORA_MODULES_TO_SAVE [LORA_MODULES_TO_SAVE ...]]
                       [--lora-register-forward-hook LORA_REGISTER_FORWARD_HOOK [LORA_REGISTER_FORWARD_HOOK ...]]
                       [--lora-fusion] [--is-instruction-dataset]
                       [--full-shuffle-instruction-dataset]
                       [--variable-seq-lengths]
                       [--tokenizer-kwargs TOKENIZER_KWARGS [TOKENIZER_KWARGS ...]]
                       [--tokenizer-padding-side TOKENIZER_PADDING_SIDE]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,Llama2Tokenizer,PretrainedFromHF,NullTokenizer}]
                       [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                       [--tokenizer-not-use-fast] [--input-layernorm-in-fp32]
                       [--no-shuffle] [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-load-balancing-type {aux_loss,group_limited_greedy,softmax_topk,pai_megatron_aux_loss}]
                       [--expert-interval EXPERT_INTERVAL]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
                       [--use-fused-moe-token-permute-and-unpermute]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-token-dispatcher-type {allgather,alltoall}]
                       [--noisy-gate-policy NOISY_GATE_POLICY]
                       [--enable-token-rearrange-opt]
                       [--embedding-multiplier-scale EMBEDDING_MULTIPLIER_SCALE]
                       [--input-jitter] [--post-norm]
                       [--output-multiplier-scale OUTPUT_MULTIPLIER_SCALE]
                       [--moe-permutation-async-comm] [--shared-expert-gate]
                       [--shared-expert-gate-output-dimension SHARED_EXPERT_GATE_OUTPUT_DIMENSION]
                       [--num-layer-list NUM_LAYER_LIST]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--profile-level {level0,level1,level2}]
                       [--profile-with-stack] [--profile-with-memory]
                       [--profile-record-shapes] [--profile-with-cpu]
                       [--profile-save-path PROFILE_SAVE_PATH]
                       [--add-qkv-bias] [--add-dense-bias] [--skip-bias-add]
                       [--add-rmsnorm-offset] [--geglu] [--input-embeds-norm]
                       [--gelu-tanh]
                       [--output-logit-softcapping OUTPUT_LOGIT_SOFTCAPPING]
                       [--attn-logit-softcapping ATTN_LOGIT_SOFTCAPPING]
                       [--query-pre-attn-scalar QUERY_PRE_ATTN_SCALAR]
                       [--interleave-sliding-window INTERLEAVE_SLIDING_WINDOW]
                       [--stage {sft,dpo,rm}]
                       [--no-gradient-accumulation-fusion]
                       [--transformer-impl {local,transformer_engine}]
                       [--enable-recompute-layers-per-pp-rank]
                       [--pre-tockens PRE_TOCKENS]
                       [--next-tockens NEXT_TOCKENS]
                       [--sparse-mode SPARSE_MODE]
                       [--shape-order {SBH,BSH,BSND,BNSD}] [--use-deter-comp]
                       [--jit-compile]
                       [--prompt-type {default,empty,chatglm2,chatglm3,chatglm3_system,glm4,chatml,chatml_de,qwen,llama3,llama2,mistral,mixtral,gemma,alpaca,deepseek2,deepseek2-lite,cpm,baichuan2}]
                       [--pad-to-multiple-of PAD_TO_MULTIPLE_OF]
                       [--scale-emb SCALE_EMB]
                       [--dim-model-base DIM_MODEL_BASE] [--no-cut-token]
                       [--scale-depth SCALE_DEPTH] [--swap-attention]
                       [--swap-modules SWAP_MODULES]
                       [--load-checkpoint-loosely] [--no-post-layer-norm]
                       [--local-rank LOCAL_RANK]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--rotary-base ROTARY_BASE]
                       [--rope-scaling-type {llama3,yarn}]
                       [--low-freq-factor LOW_FREQ_FACTOR]
                       [--high-freq-factor HIGH_FREQ_FACTOR]
                       [--original-max-position-embeddings ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--reuse-fp32-param] [--recompute-activation-function]
                       [--recompute-activation-function-num-layers RECOMPUTE_ACTIVATION_FUNCTION_NUM_LAYERS]
                       [--recompute-in-advance] [--square-alibi-mask]
                       [--fill-neg-inf] [--no-shared-storage]
                       [--enable-high-availability]
                       [--enable-optimizer-state-local-copy]
                       [--enable-hbmfault-repair]
                       [--context-parallel-algo {ulysses_cp_algo,megatron_cp_algo,hybrid_cp_algo,adaptive_cp_algo,hybrid_adaptive_cp_algo}]
                       [--ulysses-degree-in-cp ULYSSES_DEGREE_IN_CP]
                       [--cp-attention-mask-type {causal,general}]
                       [--use-cp-send-recv-overlap]
                       [--cp-window-size CP_WINDOW_SIZE]
                       [--attention-mask-on-cpu]
                       [--adaptive-cp-without-coarse]
                       [--adaptive-cp-dynamic-attn-mask]
                       [--adaptive-cp-only-reschedule]
                       [--adaptive-cp-manually-set-mask-list]
                       [--kv-head-repeat-before-uly-alltoall]
                       [--multi-head-latent-attention]
                       [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK] [--v-head-dim V_HEAD_DIM]
                       [--qk-rope-head-dim QK_ROPE_HEAD_DIM]
                       [--qk-nope-head-dim QK_NOPE_HEAD_DIM]
                       [--rope-scaling-beta-fast ROPE_SCALING_BETA_FAST]
                       [--rope-scaling-beta-slow ROPE_SCALING_BETA_SLOW]
                       [--rope-scaling-factor ROPE_SCALING_FACTOR]
                       [--rope-scaling-mscale ROPE_SCALING_MSCALE]
                       [--rope-scaling-mscale-all-dim ROPE_SCALING_MSCALE_ALL_DIM]
                       [--rope-scaling-original-max-position-embeddings ROPE_SCALING_ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--moe-intermediate-size MOE_INTERMEDIATE_SIZE]
                       [--n-shared-experts N_SHARED_EXPERTS]
                       [--topk-group TOPK_GROUP]
                       [--routed-scaling-factor ROUTED_SCALING_FACTOR]
                       [--norm-topk-prob] [--seq-aux]
                       [--first-k-dense-replace FIRST_K_DENSE_REPLACE]
                       [--moe-layer-freq MOE_LAYER_FREQ]
                       [--moe-device-level-aux-loss-coeff MOE_DEVICE_LEVEL_AUX_LOSS_COEFF]
                       [--moe-comm-aux-loss-coeff MOE_COMM_AUX_LOSS_COEFF]
                       [--dpo-beta DPO_BETA]
                       [--dpo-loss-type {sigmoid,hinge,ipo}]
                       [--dpo-ftx DPO_FTX] [--ref-model REF_MODEL]
                       [--dpo-label-smoothing DPO_LABEL_SMOOTHING]
                       [--pref-ftx PREF_FTX] [--is-pairwise-dataset]
pretrain_gpt.py: error: unrecognized arguments: \ \ ### \ ### \ ## \ ## \ ## \ ## \ \ ##
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpq0o64t5t'>
  _warnings.warn(warn_message, ResourceWarning)
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,alibi,alibi}]
                       [--use-rotary-position-embeddings]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--no-position-embedding]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--use-flash-attn]
                       [--disable-bias-linear] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--use-mcore-models] [--manual-gc]
                       [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format {zarr,torch_dist}]
                       [--ckpt-fully-parallel-save] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--overlap-grad-reduce] [--no-delay-grad-reduce]
                       [--overlap-param-gather] [--delay-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS] [--moe-grouped-gemm]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dropping] [--moe-per-layer-logging]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR] [--enable-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-entity ONE_LOGGER_ENTITY]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--yaml-cfg YAML_CFG] [--flexpipe FLEXPIPE]
                       [--flexpipe-config FLEXPIPE_CONFIG]
                       [--interleave-factor INTERLEAVE_FACTOR]
                       [--checkpoint-stages CHECKPOINT_STAGES [CHECKPOINT_STAGES ...]]
                       [--log-path LOG_PATH] [--prof-op]
                       [--prof-tp-size PROF_TP_SIZE] [--prof-path PROF_PATH]
                       [--prof-cache-file PROF_CACHE_FILE]
                       [--prof-model-name PROF_MODEL_NAME]
                       [--prof-model-size PROF_MODEL_SIZE] [--prof-time-only]
                       [--prof-memory-only]
                       [--prof-warmup-times PROF_WARMUP_TIMES]
                       [--prof-repeat-times PROF_REPEAT_TIMES [PROF_REPEAT_TIMES ...]]
                       [--prof-warmup-threshold PROF_WARMUP_THRESHOLD]
                       [--prof-repeat-threshold PROF_REPEAT_THRESHOLD]
                       [--prof-skip-running] [--prof-num-nodes PROF_NUM_NODES]
                       [--prof-node-rank PROF_NODE_RANK]
                       [--prof-ref-data PROF_REF_DATA]
                       [--prof-mbs-list PROF_MBS_LIST [PROF_MBS_LIST ...]]
                       [--use-fused-rmsnorm] [--use-fused-swiglu]
                       [--use-fused-rotary-pos-emb]
                       [--use-fused-ring-attention-update] [--use-mc2]
                       [--padded-vocab-size PADDED_VOCAB_SIZE]
                       [--embed-layernorm] [--use-glm-rope]
                       [--sliding-window SLIDING_WINDOW]
                       [--output-layer-slice-num OUTPUT_LAYER_SLICE_NUM]
                       [--lora-target-modules LORA_TARGET_MODULES [LORA_TARGET_MODULES ...]]
                       [--lora-load LORA_LOAD] [--lora-r LORA_R]
                       [--lora-alpha LORA_ALPHA]
                       [--lora-modules-to-save LORA_MODULES_TO_SAVE [LORA_MODULES_TO_SAVE ...]]
                       [--lora-register-forward-hook LORA_REGISTER_FORWARD_HOOK [LORA_REGISTER_FORWARD_HOOK ...]]
                       [--lora-fusion] [--is-instruction-dataset]
                       [--full-shuffle-instruction-dataset]
                       [--variable-seq-lengths]
                       [--tokenizer-kwargs TOKENIZER_KWARGS [TOKENIZER_KWARGS ...]]
                       [--tokenizer-padding-side TOKENIZER_PADDING_SIDE]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,Llama2Tokenizer,PretrainedFromHF,NullTokenizer}]
                       [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                       [--tokenizer-not-use-fast] [--input-layernorm-in-fp32]
                       [--no-shuffle] [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-load-balancing-type {aux_loss,group_limited_greedy,softmax_topk,pai_megatron_aux_loss}]
                       [--expert-interval EXPERT_INTERVAL]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
                       [--use-fused-moe-token-permute-and-unpermute]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-token-dispatcher-type {allgather,alltoall}]
                       [--noisy-gate-policy NOISY_GATE_POLICY]
                       [--enable-token-rearrange-opt]
                       [--embedding-multiplier-scale EMBEDDING_MULTIPLIER_SCALE]
                       [--input-jitter] [--post-norm]
                       [--output-multiplier-scale OUTPUT_MULTIPLIER_SCALE]
                       [--moe-permutation-async-comm] [--shared-expert-gate]
                       [--shared-expert-gate-output-dimension SHARED_EXPERT_GATE_OUTPUT_DIMENSION]
                       [--num-layer-list NUM_LAYER_LIST]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--profile-level {level0,level1,level2}]
                       [--profile-with-stack] [--profile-with-memory]
                       [--profile-record-shapes] [--profile-with-cpu]
                       [--profile-save-path PROFILE_SAVE_PATH]
                       [--add-qkv-bias] [--add-dense-bias] [--skip-bias-add]
                       [--add-rmsnorm-offset] [--geglu] [--input-embeds-norm]
                       [--gelu-tanh]
                       [--output-logit-softcapping OUTPUT_LOGIT_SOFTCAPPING]
                       [--attn-logit-softcapping ATTN_LOGIT_SOFTCAPPING]
                       [--query-pre-attn-scalar QUERY_PRE_ATTN_SCALAR]
                       [--interleave-sliding-window INTERLEAVE_SLIDING_WINDOW]
                       [--stage {sft,dpo,rm}]
                       [--no-gradient-accumulation-fusion]
                       [--transformer-impl {local,transformer_engine}]
                       [--enable-recompute-layers-per-pp-rank]
                       [--pre-tockens PRE_TOCKENS]
                       [--next-tockens NEXT_TOCKENS]
                       [--sparse-mode SPARSE_MODE]
                       [--shape-order {SBH,BSH,BSND,BNSD}] [--use-deter-comp]
                       [--jit-compile]
                       [--prompt-type {default,empty,chatglm2,chatglm3,chatglm3_system,glm4,chatml,chatml_de,qwen,llama3,llama2,mistral,mixtral,gemma,alpaca,deepseek2,deepseek2-lite,cpm,baichuan2}]
                       [--pad-to-multiple-of PAD_TO_MULTIPLE_OF]
                       [--scale-emb SCALE_EMB]
                       [--dim-model-base DIM_MODEL_BASE] [--no-cut-token]
                       [--scale-depth SCALE_DEPTH] [--swap-attention]
                       [--swap-modules SWAP_MODULES]
                       [--load-checkpoint-loosely] [--no-post-layer-norm]
                       [--local-rank LOCAL_RANK]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--rotary-base ROTARY_BASE]
                       [--rope-scaling-type {llama3,yarn}]
                       [--low-freq-factor LOW_FREQ_FACTOR]
                       [--high-freq-factor HIGH_FREQ_FACTOR]
                       [--original-max-position-embeddings ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--reuse-fp32-param] [--recompute-activation-function]
                       [--recompute-activation-function-num-layers RECOMPUTE_ACTIVATION_FUNCTION_NUM_LAYERS]
                       [--recompute-in-advance] [--square-alibi-mask]
                       [--fill-neg-inf] [--no-shared-storage]
                       [--enable-high-availability]
                       [--enable-optimizer-state-local-copy]
                       [--enable-hbmfault-repair]
                       [--context-parallel-algo {ulysses_cp_algo,megatron_cp_algo,hybrid_cp_algo,adaptive_cp_algo,hybrid_adaptive_cp_algo}]
                       [--ulysses-degree-in-cp ULYSSES_DEGREE_IN_CP]
                       [--cp-attention-mask-type {causal,general}]
                       [--use-cp-send-recv-overlap]
                       [--cp-window-size CP_WINDOW_SIZE]
                       [--attention-mask-on-cpu]
                       [--adaptive-cp-without-coarse]
                       [--adaptive-cp-dynamic-attn-mask]
                       [--adaptive-cp-only-reschedule]
                       [--adaptive-cp-manually-set-mask-list]
                       [--kv-head-repeat-before-uly-alltoall]
                       [--multi-head-latent-attention]
                       [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK] [--v-head-dim V_HEAD_DIM]
                       [--qk-rope-head-dim QK_ROPE_HEAD_DIM]
                       [--qk-nope-head-dim QK_NOPE_HEAD_DIM]
                       [--rope-scaling-beta-fast ROPE_SCALING_BETA_FAST]
                       [--rope-scaling-beta-slow ROPE_SCALING_BETA_SLOW]
                       [--rope-scaling-factor ROPE_SCALING_FACTOR]
                       [--rope-scaling-mscale ROPE_SCALING_MSCALE]
                       [--rope-scaling-mscale-all-dim ROPE_SCALING_MSCALE_ALL_DIM]
                       [--rope-scaling-original-max-position-embeddings ROPE_SCALING_ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--moe-intermediate-size MOE_INTERMEDIATE_SIZE]
                       [--n-shared-experts N_SHARED_EXPERTS]
                       [--topk-group TOPK_GROUP]
                       [--routed-scaling-factor ROUTED_SCALING_FACTOR]
                       [--norm-topk-prob] [--seq-aux]
                       [--first-k-dense-replace FIRST_K_DENSE_REPLACE]
                       [--moe-layer-freq MOE_LAYER_FREQ]
                       [--moe-device-level-aux-loss-coeff MOE_DEVICE_LEVEL_AUX_LOSS_COEFF]
                       [--moe-comm-aux-loss-coeff MOE_COMM_AUX_LOSS_COEFF]
                       [--dpo-beta DPO_BETA]
                       [--dpo-loss-type {sigmoid,hinge,ipo}]
                       [--dpo-ftx DPO_FTX] [--ref-model REF_MODEL]
                       [--dpo-label-smoothing DPO_LABEL_SMOOTHING]
                       [--pref-ftx PREF_FTX] [--is-pairwise-dataset]
pretrain_gpt.py: error: unrecognized arguments: \ \ ### \ ### \ ## \ ## \ ## \ ## \ \ ##
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmppwkzlz7r'>
  _warnings.warn(warn_message, ResourceWarning)
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--encoder-num-layers ENCODER_NUM_LAYERS]
                       [--decoder-num-layers DECODER_NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS] [--group-query-attention]
                       [--num-query-groups NUM_QUERY_GROUPS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--position-embedding-type {learned_absolute,rope,alibi,alibi}]
                       [--use-rotary-position-embeddings]
                       [--rotary-percent ROTARY_PERCENT]
                       [--rotary-interleaved]
                       [--rotary-seq-len-interpolation-factor ROTARY_SEQ_LEN_INTERPOLATION_FACTOR]
                       [--no-position-embedding]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--normalization {LayerNorm,RMSNorm}]
                       [--norm-epsilon NORM_EPSILON] [--apply-layernorm-1p]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--squared-relu] [--swiglu]
                       [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
                       [--untie-embeddings-and-output-weights]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY]
                       [--start-weight-decay START_WEIGHT_DECAY]
                       [--end-weight-decay END_WEIGHT_DECAY]
                       [--weight-decay-incr-style {constant,linear,cosine}]
                       [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
                       [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
                       [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--recompute-activations]
                       [--recompute-granularity {full,selective}]
                       [--no-check-for-nan-in-loss-and-grad]
                       [--distribute-saved-activations]
                       [--recompute-method {uniform,block}]
                       [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
                       [--no-clone-scatter-output-in-embedding] [--profile]
                       [--profile-step-start PROFILE_STEP_START]
                       [--profile-step-end PROFILE_STEP_END]
                       [--tp-comm-overlap]
                       [--tp-comm-overlap-cfg TP_COMM_OVERLAP_CFG]
                       [--disable-tp-comm-overlap-ag]
                       [--disable-tp-comm-overlap-rs]
                       [--disable-tp-comm-bulk-dgrad]
                       [--disable-tp-comm-bulk-wgrad]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--checkpoint-activations] [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--exit-signal-handler]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-swiglu-fusion] [--no-bias-dropout-fusion]
                       [--no-rope-fusion] [--use-flash-attn]
                       [--disable-bias-linear] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic,external}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--no-persist-layer-norm] [--sequence-parallel]
                       [--use-mcore-models] [--manual-gc]
                       [--manual-gc-interval MANUAL_GC_INTERVAL]
                       [--no-manual-gc-eval] [--disable-tp-comm-split-ag]
                       [--disable-tp-comm-split-rs] [--seed SEED]
                       [--data-parallel-random-init]
                       [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--lr-warmup-init LR_WARMUP_INIT] [--warmup WARMUP]
                       [--min-lr MIN_LR] [--override-opt_param-scheduler]
                       [--use-checkpoint-opt_param-scheduler]
                       [--decoupled-lr DECOUPLED_LR]
                       [--decoupled-min-lr DECOUPLED_MIN_LR] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune]
                       [--pretrained-checkpoint PRETRAINED_CHECKPOINT]
                       [--ckpt-step CKPT_STEP] [--no-initialization]
                       [--use-checkpoint-args] [--exit-on-missing-checkpoint]
                       [--use-dist-ckpt] [--auto-detect-ckpt-format]
                       [--dist-ckpt-format {zarr,torch_dist}]
                       [--ckpt-fully-parallel-save] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--apply-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--no-overlap-p2p-communication]
                       [--distributed-backend {nccl,gloo}]
                       [--overlap-grad-reduce] [--no-delay-grad-reduce]
                       [--overlap-param-gather] [--delay-param-gather]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--use-ring-exchange-p2p] [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--standalone-embedding-stage]
                       [--use-distributed-optimizer]
                       [--context-parallel-size CONTEXT_PARALLEL_SIZE]
                       [--nccl-communicator-config-path NCCL_COMMUNICATOR_CONFIG_PATH]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL] [--test-mode]
                       [--skip-train] [--data-path [DATA_PATH ...]]
                       [--split SPLIT]
                       [--train-data-path [TRAIN_DATA_PATH ...]]
                       [--valid-data-path [VALID_DATA_PATH ...]]
                       [--test-data-path [TEST_DATA_PATH ...]]
                       [--data-cache-path DATA_CACHE_PATH]
                       [--no-mmap-bin-files] [--mock-data]
                       [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
                       [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-model TOKENIZER_MODEL]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss]
                       [--no-create-attention-mask-in-dataloader]
                       [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-h IMG_H]
                       [--img-w IMG_W] [--num-channels NUM_CHANNELS]
                       [--patch-dim PATCH_DIM]
                       [--classes-fraction CLASSES_FRACTION]
                       [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
                       [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
                       [--vision-pretraining]
                       [--vision-pretraining-type {classify,inpaint,dino}]
                       [--vision-backbone-type {vit,mit,swin}]
                       [--swin-backbone-type {tiny,base,h3}]
                       [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
                       [--iter-per-epoch ITER_PER_EPOCH]
                       [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
                       [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
                       [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
                       [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
                       [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
                       [--dino-norm-last-layer]
                       [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
                       [--dino-teacher-temp DINO_TEACHER_TEMP]
                       [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
                       [--qk-layernorm]
                       [--expert-model-parallel-size EXPERT_MODEL_PARALLEL_SIZE]
                       [--num-experts NUM_EXPERTS] [--moe-grouped-gemm]
                       [--moe-input-jitter-eps MOE_INPUT_JITTER_EPS]
                       [--moe-token-dropping] [--moe-per-layer-logging]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--log-throughput] [--log-progress]
                       [--timing-log-level {0,1,2}]
                       [--no-barrier-with-level-1-timing]
                       [--timing-log-option {max,minmax,all}]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard]
                       [--log-world-size-to-tensorboard]
                       [--wandb-project WANDB_PROJECT]
                       [--wandb-exp-name WANDB_EXP_NAME]
                       [--wandb-save-dir WANDB_SAVE_DIR] [--enable-one-logger]
                       [--one-logger-project ONE_LOGGER_PROJECT]
                       [--one-logger-entity ONE_LOGGER_ENTITY]
                       [--one-logger-run-name ONE_LOGGER_RUN_NAME]
                       [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
                       [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
                       [--output-bert-embeddings]
                       [--bert-embedder-type {megatron,huggingface}]
                       [--fp8-format {e4m3,hybrid}] [--fp8-margin FP8_MARGIN]
                       [--fp8-interval FP8_INTERVAL]
                       [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
                       [--fp8-amax-compute-algo {most_recent,max}]
                       [--no-fp8-wgrad]
                       [--retro-project-dir RETRO_PROJECT_DIR]
                       [--retro-add-retriever]
                       [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
                       [--retro-encoder-layers RETRO_ENCODER_LAYERS]
                       [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
                       [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
                       [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
                       [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
                       [--retro-attention-gate RETRO_ATTENTION_GATE]
                       [--retro-no-verify-neighbor-count] [--spec [SPEC ...]]
                       [--yaml-cfg YAML_CFG] [--flexpipe FLEXPIPE]
                       [--flexpipe-config FLEXPIPE_CONFIG]
                       [--interleave-factor INTERLEAVE_FACTOR]
                       [--checkpoint-stages CHECKPOINT_STAGES [CHECKPOINT_STAGES ...]]
                       [--log-path LOG_PATH] [--prof-op]
                       [--prof-tp-size PROF_TP_SIZE] [--prof-path PROF_PATH]
                       [--prof-cache-file PROF_CACHE_FILE]
                       [--prof-model-name PROF_MODEL_NAME]
                       [--prof-model-size PROF_MODEL_SIZE] [--prof-time-only]
                       [--prof-memory-only]
                       [--prof-warmup-times PROF_WARMUP_TIMES]
                       [--prof-repeat-times PROF_REPEAT_TIMES [PROF_REPEAT_TIMES ...]]
                       [--prof-warmup-threshold PROF_WARMUP_THRESHOLD]
                       [--prof-repeat-threshold PROF_REPEAT_THRESHOLD]
                       [--prof-skip-running] [--prof-num-nodes PROF_NUM_NODES]
                       [--prof-node-rank PROF_NODE_RANK]
                       [--prof-ref-data PROF_REF_DATA]
                       [--prof-mbs-list PROF_MBS_LIST [PROF_MBS_LIST ...]]
                       [--use-fused-rmsnorm] [--use-fused-swiglu]
                       [--use-fused-rotary-pos-emb]
                       [--use-fused-ring-attention-update] [--use-mc2]
                       [--padded-vocab-size PADDED_VOCAB_SIZE]
                       [--embed-layernorm] [--use-glm-rope]
                       [--sliding-window SLIDING_WINDOW]
                       [--output-layer-slice-num OUTPUT_LAYER_SLICE_NUM]
                       [--lora-target-modules LORA_TARGET_MODULES [LORA_TARGET_MODULES ...]]
                       [--lora-load LORA_LOAD] [--lora-r LORA_R]
                       [--lora-alpha LORA_ALPHA]
                       [--lora-modules-to-save LORA_MODULES_TO_SAVE [LORA_MODULES_TO_SAVE ...]]
                       [--lora-register-forward-hook LORA_REGISTER_FORWARD_HOOK [LORA_REGISTER_FORWARD_HOOK ...]]
                       [--lora-fusion] [--is-instruction-dataset]
                       [--full-shuffle-instruction-dataset]
                       [--variable-seq-lengths]
                       [--tokenizer-kwargs TOKENIZER_KWARGS [TOKENIZER_KWARGS ...]]
                       [--tokenizer-padding-side TOKENIZER_PADDING_SIDE]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,Llama2Tokenizer,PretrainedFromHF,NullTokenizer}]
                       [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                       [--tokenizer-not-use-fast] [--input-layernorm-in-fp32]
                       [--no-shuffle] [--moe-router-topk MOE_ROUTER_TOPK]
                       [--moe-router-load-balancing-type {aux_loss,group_limited_greedy,softmax_topk,pai_megatron_aux_loss}]
                       [--expert-interval EXPERT_INTERVAL]
                       [--moe-aux-loss-coeff MOE_AUX_LOSS_COEFF]
                       [--moe-z-loss-coeff MOE_Z_LOSS_COEFF]
                       [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
                       [--use-fused-moe-token-permute-and-unpermute]
                       [--moe-expert-capacity-factor MOE_EXPERT_CAPACITY_FACTOR]
                       [--moe-pad-expert-input-to-capacity]
                       [--moe-token-drop-policy {probs,position}]
                       [--moe-token-dispatcher-type {allgather,alltoall}]
                       [--noisy-gate-policy NOISY_GATE_POLICY]
                       [--enable-token-rearrange-opt]
                       [--embedding-multiplier-scale EMBEDDING_MULTIPLIER_SCALE]
                       [--input-jitter] [--post-norm]
                       [--output-multiplier-scale OUTPUT_MULTIPLIER_SCALE]
                       [--moe-permutation-async-comm] [--shared-expert-gate]
                       [--shared-expert-gate-output-dimension SHARED_EXPERT_GATE_OUTPUT_DIMENSION]
                       [--num-layer-list NUM_LAYER_LIST]
                       [--profile-ranks PROFILE_RANKS [PROFILE_RANKS ...]]
                       [--profile-level {level0,level1,level2}]
                       [--profile-with-stack] [--profile-with-memory]
                       [--profile-record-shapes] [--profile-with-cpu]
                       [--profile-save-path PROFILE_SAVE_PATH]
                       [--add-qkv-bias] [--add-dense-bias] [--skip-bias-add]
                       [--add-rmsnorm-offset] [--geglu] [--input-embeds-norm]
                       [--gelu-tanh]
                       [--output-logit-softcapping OUTPUT_LOGIT_SOFTCAPPING]
                       [--attn-logit-softcapping ATTN_LOGIT_SOFTCAPPING]
                       [--query-pre-attn-scalar QUERY_PRE_ATTN_SCALAR]
                       [--interleave-sliding-window INTERLEAVE_SLIDING_WINDOW]
                       [--stage {sft,dpo,rm}]
                       [--no-gradient-accumulation-fusion]
                       [--transformer-impl {local,transformer_engine}]
                       [--enable-recompute-layers-per-pp-rank]
                       [--pre-tockens PRE_TOCKENS]
                       [--next-tockens NEXT_TOCKENS]
                       [--sparse-mode SPARSE_MODE]
                       [--shape-order {SBH,BSH,BSND,BNSD}] [--use-deter-comp]
                       [--jit-compile]
                       [--prompt-type {default,empty,chatglm2,chatglm3,chatglm3_system,glm4,chatml,chatml_de,qwen,llama3,llama2,mistral,mixtral,gemma,alpaca,deepseek2,deepseek2-lite,cpm,baichuan2}]
                       [--pad-to-multiple-of PAD_TO_MULTIPLE_OF]
                       [--scale-emb SCALE_EMB]
                       [--dim-model-base DIM_MODEL_BASE] [--no-cut-token]
                       [--scale-depth SCALE_DEPTH] [--swap-attention]
                       [--swap-modules SWAP_MODULES]
                       [--load-checkpoint-loosely] [--no-post-layer-norm]
                       [--local-rank LOCAL_RANK]
                       [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
                       [--rotary-base ROTARY_BASE]
                       [--rope-scaling-type {llama3,yarn}]
                       [--low-freq-factor LOW_FREQ_FACTOR]
                       [--high-freq-factor HIGH_FREQ_FACTOR]
                       [--original-max-position-embeddings ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--reuse-fp32-param] [--recompute-activation-function]
                       [--recompute-activation-function-num-layers RECOMPUTE_ACTIVATION_FUNCTION_NUM_LAYERS]
                       [--recompute-in-advance] [--square-alibi-mask]
                       [--fill-neg-inf] [--no-shared-storage]
                       [--enable-high-availability]
                       [--enable-optimizer-state-local-copy]
                       [--enable-hbmfault-repair]
                       [--context-parallel-algo {ulysses_cp_algo,megatron_cp_algo,hybrid_cp_algo,adaptive_cp_algo,hybrid_adaptive_cp_algo}]
                       [--ulysses-degree-in-cp ULYSSES_DEGREE_IN_CP]
                       [--cp-attention-mask-type {causal,general}]
                       [--use-cp-send-recv-overlap]
                       [--cp-window-size CP_WINDOW_SIZE]
                       [--attention-mask-on-cpu]
                       [--adaptive-cp-without-coarse]
                       [--adaptive-cp-dynamic-attn-mask]
                       [--adaptive-cp-only-reschedule]
                       [--adaptive-cp-manually-set-mask-list]
                       [--kv-head-repeat-before-uly-alltoall]
                       [--multi-head-latent-attention]
                       [--q-lora-rank Q_LORA_RANK]
                       [--kv-lora-rank KV_LORA_RANK] [--v-head-dim V_HEAD_DIM]
                       [--qk-rope-head-dim QK_ROPE_HEAD_DIM]
                       [--qk-nope-head-dim QK_NOPE_HEAD_DIM]
                       [--rope-scaling-beta-fast ROPE_SCALING_BETA_FAST]
                       [--rope-scaling-beta-slow ROPE_SCALING_BETA_SLOW]
                       [--rope-scaling-factor ROPE_SCALING_FACTOR]
                       [--rope-scaling-mscale ROPE_SCALING_MSCALE]
                       [--rope-scaling-mscale-all-dim ROPE_SCALING_MSCALE_ALL_DIM]
                       [--rope-scaling-original-max-position-embeddings ROPE_SCALING_ORIGINAL_MAX_POSITION_EMBEDDINGS]
                       [--moe-intermediate-size MOE_INTERMEDIATE_SIZE]
                       [--n-shared-experts N_SHARED_EXPERTS]
                       [--topk-group TOPK_GROUP]
                       [--routed-scaling-factor ROUTED_SCALING_FACTOR]
                       [--norm-topk-prob] [--seq-aux]
                       [--first-k-dense-replace FIRST_K_DENSE_REPLACE]
                       [--moe-layer-freq MOE_LAYER_FREQ]
                       [--moe-device-level-aux-loss-coeff MOE_DEVICE_LEVEL_AUX_LOSS_COEFF]
                       [--moe-comm-aux-loss-coeff MOE_COMM_AUX_LOSS_COEFF]
                       [--dpo-beta DPO_BETA]
                       [--dpo-loss-type {sigmoid,hinge,ipo}]
                       [--dpo-ftx DPO_FTX] [--ref-model REF_MODEL]
                       [--dpo-label-smoothing DPO_LABEL_SMOOTHING]
                       [--pref-ftx PREF_FTX] [--is-pairwise-dataset]
pretrain_gpt.py: error: unrecognized arguments: \ \ ### \ ### \ ## \ ## \ ## \ ## \ \ ##
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp3elgp39o'>
  _warnings.warn(warn_message, ResourceWarning)
[2024-12-10 20:20:47,233] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 299807 closing signal SIGTERM
[2024-12-10 20:20:47,234] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 299808 closing signal SIGTERM
[2024-12-10 20:20:47,234] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 299809 closing signal SIGTERM
[2024-12-10 20:20:47,234] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 299810 closing signal SIGTERM
[2024-12-10 20:20:47,234] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 299811 closing signal SIGTERM
[2024-12-10 20:20:47,234] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 299812 closing signal SIGTERM
