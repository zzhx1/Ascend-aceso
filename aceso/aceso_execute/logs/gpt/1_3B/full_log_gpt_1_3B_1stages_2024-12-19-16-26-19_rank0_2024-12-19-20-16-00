[2024-12-19 20:16:03,226] torch.distributed.run: [WARNING] 
[2024-12-19 20:16:03,226] torch.distributed.run: [WARNING] *****************************************
[2024-12-19 20:16:03,226] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-12-19 20:16:03,226] torch.distributed.run: [WARNING] *****************************************
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:299: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:260: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module adaptive_cp...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
ninja: no work to do.
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Using /root/.cache/torch_extensions/py39_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py39_cpu/adaptive_cp/build.ninja...
Building extension module adaptive_cp...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module adaptive_cp...
Loading extension module adaptive_cp...
world_size: 8
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json' mode='r' encoding='UTF-8'>
  self.encoder = json.load(open(vocab_file))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt' mode='r' encoding='utf-8'>
  bpe_data = open(merges_file, encoding='utf-8').read().split('\n')[1:-1]
ResourceWarning: Enable tracemalloc to get the object allocation traceback
world_size: 8
world_size: 8
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json' mode='r' encoding='UTF-8'>
  self.encoder = json.load(open(vocab_file))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt' mode='r' encoding='utf-8'>
  bpe_data = open(merges_file, encoding='utf-8').read().split('\n')[1:-1]
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json' mode='r' encoding='UTF-8'>
  self.encoder = json.load(open(vocab_file))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt' mode='r' encoding='utf-8'>
  bpe_data = open(merges_file, encoding='utf-8').read().split('\n')[1:-1]
ResourceWarning: Enable tracemalloc to get the object allocation traceback
world_size: 8
world_size: 8
using world size: 8, context-parallel size: 1
--checkpoint-activations is no longer valid, use --recompute-activations, or, for more control, --recompute-granularity and --recompute-method.
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
WARNING: Setting args.check_for_nan_in_loss_and_grad to False since dynamic loss scaling is being used
using torch.float16 for parameters ...
When context_parallel is not activated, kv_head_repeat_before_uly_alltoall would be set to False for reducing memory usage.
[INFO] Setting args.create_attention_mask_in_dataloader to False since reset_data=False or alibi_without_flash_attn=False or args.tokenizer_padding_side=right
------------------------ ModelLink Arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adaptive_cp_dynamic_attn_mask ................... False
  adaptive_cp_manually_set_mask_list .............. False
  adaptive_cp_only_reschedule ..................... False
  adaptive_cp_without_coarse ...................... False
  adaptive_recompute_device_size .................. -1
  adaptive_recompute_device_swap .................. False
  adaptive_recompute_profiling_step ............... 10
  add_bias_linear ................................. True
  add_dense_bias .................................. False
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  add_rmsnorm_offset .............................. False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  algo_of_each_op ................................. [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_mask_on_cpu ........................... False
  attention_softmax_in_fp32 ....................... True
  attn_logit_softcapping .......................... None
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. False
  checkpoint_stages ............................... []
  ckpt_fully_parallel_save ........................ False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_algo ........................... megatron_cp_algo
  context_parallel_size ........................... 1
  cp_attention_mask_type .......................... causal
  cp_window_size .................................. 1
  create_attention_mask_in_dataloader ............. False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size_of_each_op ................... [[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]]
  data_path ....................................... None
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dim_model_base .................................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  dist_ckpt_format ................................ torch_dist
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 45
  dpo_beta ........................................ 0.1
  dpo_ftx ......................................... 0.0
  dpo_label_smoothing ............................. 0.0
  dpo_loss_type ................................... sigmoid
  embed_layernorm ................................. False
  embedding_multiplier_scale ...................... 1.0
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_hbmfault_repair .......................... False
  enable_high_availability ........................ False
  enable_one_logger ............................... False
  enable_optimizer_state_local_copy ............... False
  enable_recompute_layers_per_pp_rank ............. False
  enable_token_rearrange_opt ...................... False
  encoder_num_layers .............................. 24
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 2000
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_interval ................................. 1
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 8192
  fill_neg_inf .................................... False
  finetune ........................................ False
  first_k_dense_replace ........................... None
  flexpipe ........................................ True
  flexpipe_config ................................. /workspace/Ascend-aceso/aceso/aceso_execute/../search_configs/configs/gpt/1_3B/top_configs/gpt_1_3B_1stages_2024-12-19-16-26-19.json
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  full_shuffle_instruction_dataset ................ False
  geglu ........................................... False
  gelu_tanh ....................................... False
  global_batch_size ............................... 1024
  gradient_accumulation_fusion .................... False
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 2048
  high_freq_factor ................................ None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4096.0
  input_embeds_norm ............................... False
  input_jitter .................................... True
  input_layernorm_in_fp32 ......................... False
  interleave_factor ............................... 1
  interleave_sliding_window ....................... None
  is_instruction_dataset .......................... False
  is_pairwise_dataset ............................. False
  iter_per_epoch .................................. 1250
  jit_compile ..................................... False
  kv_channels ..................................... 64
  kv_head_repeat_before_uly_alltoall .............. False
  kv_lora_rank .................................... None
  lazy_mpu_init ................................... None
  load ............................................ None
  load_checkpoint_loosely ......................... False
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_name ........................................ gpt_1_3B_1stages_2024-12-19-16-26-19
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_path ........................................ /workspace/Ascend-aceso/aceso/aceso_execute/logs/gpt/1_3B/
  log_progress .................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  lora_alpha ...................................... 32
  lora_fusion ..................................... False
  lora_load ....................................... None
  lora_modules_to_save ............................ None
  lora_r .......................................... 16
  lora_register_forward_hook ...................... ['word_embeddings', 'input_layernorm']
  lora_target_modules ............................. []
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  low_freq_factor ................................. None
  lr .............................................. 6e-05
  lr_decay_iters .................................. 320000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.001
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt
  micro_batch_size ................................ 32
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mmap_bin_files .................................. True
  mock_data ....................................... True
  moe_allgather_overlap_comm ...................... False
  moe_alltoall_overlap_comm ....................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_comm_aux_loss_coeff ......................... 0.0
  moe_device_level_aux_loss_coeff ................. 0.0
  moe_expert_capacity_factor ...................... None
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_intermediate_size ........................... None
  moe_layer_freq .................................. None
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_permutation_async_comm ...................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_token_dropping .............................. False
  moe_tp_extend_ep ................................ False
  moe_train_capacity_factor ....................... 1.0
  moe_without_activation .......................... False
  moe_z_loss_coeff ................................ 0.0
  multi_head_latent_attention ..................... False
  n_shared_experts ................................ None
  nccl_communicator_config_path ................... None
  next_tockens .................................... 0
  no_cut_token .................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_post_layer_norm .............................. False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  no_shared_storage ............................... True
  no_shuffle ...................................... False
  noisy_gate_policy ............................... None
  noop_layers ..................................... None
  norm_epsilon .................................... 1e-05
  norm_topk_prob .................................. False
  normalization ................................... LayerNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_gpus ........................................ [8]
  num_layer_list .................................. None
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_ops_in_each_stage ........................... [50]
  num_query_groups ................................ 1
  num_stages ...................................... 1
  num_workers ..................................... 2
  one_logger_entity ............................... hwinf_dcm
  one_logger_project .............................. e2e-tracking
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  original_max_position_embeddings ................ None
  output_bert_embeddings .......................... False
  output_layer_slice_num .......................... 1
  output_logit_softcapping ........................ None
  output_multiplier_scale ......................... None
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  pad_to_multiple_of .............................. 8
  padded_vocab_size ............................... None
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  position_embedding_type ......................... learned_absolute
  post_norm ....................................... False
  pre_tockens ..................................... 65536
  pref_ftx ........................................ 0.0
  pretrained_checkpoint ........................... None
  prof_cache_file ................................. None
  prof_mbs_list ................................... None
  prof_memory_only ................................ False
  prof_model_name ................................. all
  prof_model_size ................................. all
  prof_node_rank .................................. None
  prof_num_nodes .................................. None
  prof_op ......................................... False
  prof_path ....................................... None
  prof_ref_data ................................... None
  prof_repeat_threshold ........................... None
  prof_repeat_times ............................... [50]
  prof_skip_running ............................... False
  prof_time_only .................................. False
  prof_tp_size .................................... None
  prof_warmup_threshold ........................... None
  prof_warmup_times ............................... 20
  profile ......................................... False
  profile_level ................................... level0
  profile_ranks ................................... [-1]
  profile_record_shapes ........................... False
  profile_save_path ............................... ./profile_dir
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  profile_with_cpu ................................ False
  profile_with_memory ............................. False
  profile_with_stack .............................. False
  prompt_type ..................................... None
  q_lora_rank ..................................... None
  qk_layernorm .................................... False
  qk_nope_head_dim ................................ None
  qk_rope_head_dim ................................ None
  query_in_block_prob ............................. 0.1
  query_pre_attn_scalar ........................... None
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_activation_function ................... False
  recompute_activation_function_num_layers ........ None
  recompute_granularity ........................... None
  recompute_in_advance ............................ False
  recompute_in_bubble ............................. False
  recompute_method ................................ None
  recompute_num_layers ............................ None
  recompute_ops ................................... [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
  reduce_recompute_for_last_chunk ................. False
  ref_model ....................................... None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  resharding_stages ............................... [False]
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  reuse_fp32_param ................................ False
  rope_scaling_beta_fast .......................... 32
  rope_scaling_beta_slow .......................... 1
  rope_scaling_factor ............................. 1.0
  rope_scaling_mscale ............................. 1.0
  rope_scaling_mscale_all_dim ..................... 0.0
  rope_scaling_original_max_position_embeddings ... None
  rope_scaling_type ............................... None
  rotary_base ..................................... None
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  routed_scaling_factor ........................... None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 2000
  scale_depth ..................................... None
  scale_emb ....................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_aux ......................................... False
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  shape_order ..................................... SBH
  share_embeddings_and_output_weights ............. True
  shared_expert_gate .............................. False
  shared_expert_gate_output_dimension ............. 1
  short_seq_prob .................................. 0.1
  skip_bias_add ................................... True
  skip_train ...................................... False
  sliding_window .................................. None
  sparse_mode ..................................... 0
  spec ............................................ None
  split ........................................... 949,50,1
  square_alibi_mask ............................... False
  squared_relu .................................... False
  stage ........................................... None
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  swap_attention .................................. False
  swap_modules .................................... None
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_parallel_size_of_each_op ................. [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  timing_log_level ................................ 2
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_kwargs ................................ None
  tokenizer_model ................................. None
  tokenizer_name_or_path .......................... None
  tokenizer_not_use_fast .......................... True
  tokenizer_padding_side .......................... right
  tokenizer_type .................................. GPT2BPETokenizer
  topk_group ...................................... None
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 18
  train_samples ................................... None
  transformer_impl ................................ local
  ulysses_degree_in_cp ............................ None
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cp_send_recv_overlap ........................ True
  use_cpu_initialization .......................... None
  use_deter_comp .................................. False
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. True
  use_fused_moe_token_permute_and_unpermute ....... False
  use_fused_ring_attention_update ................. False
  use_fused_rmsnorm ............................... False
  use_fused_rotary_pos_emb ........................ False
  use_fused_swiglu ................................ False
  use_glm_rope .................................... False
  use_mc2 ......................................... False
  use_mcore_models ................................ True
  use_nanopipe .................................... False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  v_head_dim ...................................... None
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ 1
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 8
  yaml_cfg ........................................ None
-------------------- end of ModelLink Arguments ---------------------
setting number of micro-batches to constant 32
> building GPT2BPETokenizer tokenizer ...
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json' mode='r' encoding='UTF-8'>
  self.encoder = json.load(open(vocab_file))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
world_size: 8
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt' mode='r' encoding='utf-8'>
  bpe_data = open(merges_file, encoding='utf-8').read().split('\n')[1:-1]
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json' mode='r' encoding='UTF-8'>
  self.encoder = json.load(open(vocab_file))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt' mode='r' encoding='utf-8'>
  bpe_data = open(merges_file, encoding='utf-8').read().split('\n')[1:-1]
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json' mode='r' encoding='UTF-8'>
  self.encoder = json.load(open(vocab_file))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt' mode='r' encoding='utf-8'>
  bpe_data = open(merges_file, encoding='utf-8').read().split('\n')[1:-1]
ResourceWarning: Enable tracemalloc to get the object allocation traceback
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
world_size: 8
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json' mode='r' encoding='UTF-8'>
  self.encoder = json.load(open(vocab_file))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt' mode='r' encoding='utf-8'>
  bpe_data = open(merges_file, encoding='utf-8').read().split('\n')[1:-1]
ResourceWarning: Enable tracemalloc to get the object allocation traceback
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
world_size: 8
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:159: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-vocab.json' mode='r' encoding='UTF-8'>
  self.encoder = json.load(open(vocab_file))
ResourceWarning: Enable tracemalloc to get the object allocation traceback
/workspace/Ascend-aceso/megatron/training/tokenizer/gpt2_tokenization.py:164: ResourceWarning: unclosed file <_io.TextIOWrapper name='/workspace/Ascend-aceso/aceso/aceso_execute/../../vocab_file/gpt2-merges.txt' mode='r' encoding='utf-8'>
  bpe_data = open(merges_file, encoding='utf-8').read().split('\n')[1:-1]
ResourceWarning: Enable tracemalloc to get the object allocation traceback
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[DEBUG]|rank 1|     pipeline_rank= 0 |     tp_size= 1 |     tp_rank=0 |     tp_src_rank=1 |     dp_size= 8 |     parent ranks=[1] |     child ranks = [1] |     micro_batch_size = 32

[DEBUG]|rank 1|     MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE: 1 |     DATA_PARALLEL_RANKS: [range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8)] |     MPU_PIPELINE_MODEL_PARALLEL_RANK: 0 |     CHILD_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     PARENT_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     FLEXPIPE_PREV_RANKS: [1] |     FLEXPIPE_NEXT_RANKS: [1] |     RANKS_IN_EACH_PIPELINE_STAGE: [[0, 1, 2, 3, 4, 5, 6, 7]]|     OP_RESHARDING_RANKS: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] |     EMBEDDING_GLOBAL_RANKS: range(0, 1) |     POSITION_EMBEDDING_GLOBAL_RANKS: range(0, 1)
[DEBUG]|rank 7|     pipeline_rank= 0 |     tp_size= 1 |     tp_rank=0 |     tp_src_rank=7 |     dp_size= 8 |     parent ranks=[7] |     child ranks = [7] |     micro_batch_size = 32

[DEBUG]|rank 7|     MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE: 1 |     DATA_PARALLEL_RANKS: [range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8)] |     MPU_PIPELINE_MODEL_PARALLEL_RANK: 0 |     CHILD_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     PARENT_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     FLEXPIPE_PREV_RANKS: [7] |     FLEXPIPE_NEXT_RANKS: [7] |     RANKS_IN_EACH_PIPELINE_STAGE: [[0, 1, 2, 3, 4, 5, 6, 7]]|     OP_RESHARDING_RANKS: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] |     EMBEDDING_GLOBAL_RANKS: range(0, 1) |     POSITION_EMBEDDING_GLOBAL_RANKS: range(0, 1)
[DEBUG]|rank 6|     pipeline_rank= 0 |     tp_size= 1 |     tp_rank=0 |     tp_src_rank=6 |     dp_size= 8 |     parent ranks=[6] |     child ranks = [6] |     micro_batch_size = 32

[DEBUG]|rank 6|     MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE: 1 |     DATA_PARALLEL_RANKS: [range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8)] |     MPU_PIPELINE_MODEL_PARALLEL_RANK: 0 |     CHILD_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     PARENT_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     FLEXPIPE_PREV_RANKS: [6] |     FLEXPIPE_NEXT_RANKS: [6] |     RANKS_IN_EACH_PIPELINE_STAGE: [[0, 1, 2, 3, 4, 5, 6, 7]]|     OP_RESHARDING_RANKS: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] |     EMBEDDING_GLOBAL_RANKS: range(0, 1) |     POSITION_EMBEDDING_GLOBAL_RANKS: range(0, 1)
> initializing FlexPipe...
[DEBUG]|rank 0|     pipeline_rank= 0 |     tp_size= 1 |     tp_rank=0 |     tp_src_rank=0 |     dp_size= 8 |     parent ranks=[0] |     child ranks = [0] |     micro_batch_size = 32

[DEBUG]|rank 0|     MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE: 1 |     DATA_PARALLEL_RANKS: [range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8)] |     MPU_PIPELINE_MODEL_PARALLEL_RANK: 0 |     CHILD_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     PARENT_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     FLEXPIPE_PREV_RANKS: [0] |     FLEXPIPE_NEXT_RANKS: [0] |     RANKS_IN_EACH_PIPELINE_STAGE: [[0, 1, 2, 3, 4, 5, 6, 7]]|     OP_RESHARDING_RANKS: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] |     EMBEDDING_GLOBAL_RANKS: range(0, 1) |     POSITION_EMBEDDING_GLOBAL_RANKS: range(0, 1)
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
[DEBUG]|rank 5|     pipeline_rank= 0 |     tp_size= 1 |     tp_rank=0 |     tp_src_rank=5 |     dp_size= 8 |     parent ranks=[5] |     child ranks = [5] |     micro_batch_size = 32

[DEBUG]|rank 5|     MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE: 1 |     DATA_PARALLEL_RANKS: [range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8)] |     MPU_PIPELINE_MODEL_PARALLEL_RANK: 0 |     CHILD_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     PARENT_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     FLEXPIPE_PREV_RANKS: [5] |     FLEXPIPE_NEXT_RANKS: [5] |     RANKS_IN_EACH_PIPELINE_STAGE: [[0, 1, 2, 3, 4, 5, 6, 7]]|     OP_RESHARDING_RANKS: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] |     EMBEDDING_GLOBAL_RANKS: range(0, 1) |     POSITION_EMBEDDING_GLOBAL_RANKS: range(0, 1)
> compiling dataset index builder ...
make: Entering directory '/workspace/Ascend-aceso/megatron/core/datasets'
[DEBUG]|rank 3|     pipeline_rank= 0 |     tp_size= 1 |     tp_rank=0 |     tp_src_rank=3 |     dp_size= 8 |     parent ranks=[3] |     child ranks = [3] |     micro_batch_size = 32

[DEBUG]|rank 3|     MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE: 1 |     DATA_PARALLEL_RANKS: [range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8)] |     MPU_PIPELINE_MODEL_PARALLEL_RANK: 0 |     CHILD_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     PARENT_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     FLEXPIPE_PREV_RANKS: [3] |     FLEXPIPE_NEXT_RANKS: [3] |     RANKS_IN_EACH_PIPELINE_STAGE: [[0, 1, 2, 3, 4, 5, 6, 7]]|     OP_RESHARDING_RANKS: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] |     EMBEDDING_GLOBAL_RANKS: range(0, 1) |     POSITION_EMBEDDING_GLOBAL_RANKS: range(0, 1)
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/Ascend-aceso/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.943 seconds
[DEBUG]|rank 2|     pipeline_rank= 0 |     tp_size= 1 |     tp_rank=0 |     tp_src_rank=2 |     dp_size= 8 |     parent ranks=[2] |     child ranks = [2] |     micro_batch_size = 32

[DEBUG]|rank 2|     MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE: 1 |     DATA_PARALLEL_RANKS: [range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8)] |     MPU_PIPELINE_MODEL_PARALLEL_RANK: 0 |     CHILD_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     PARENT_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     FLEXPIPE_PREV_RANKS: [2] |     FLEXPIPE_NEXT_RANKS: [2] |     RANKS_IN_EACH_PIPELINE_STAGE: [[0, 1, 2, 3, 4, 5, 6, 7]]|     OP_RESHARDING_RANKS: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] |     EMBEDDING_GLOBAL_RANKS: range(0, 1) |     POSITION_EMBEDDING_GLOBAL_RANKS: range(0, 1)
[DEBUG]|rank 4|     pipeline_rank= 0 |     tp_size= 1 |     tp_rank=0 |     tp_src_rank=4 |     dp_size= 8 |     parent ranks=[4] |     child ranks = [4] |     micro_batch_size = 32

[DEBUG]|rank 4|     MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE: 1 |     DATA_PARALLEL_RANKS: [range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8), range(0, 8)] |     MPU_PIPELINE_MODEL_PARALLEL_RANK: 0 |     CHILD_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     PARENT_RANKS: [[0], [1], [2], [3], [4], [5], [6], [7]] |     FLEXPIPE_PREV_RANKS: [4] |     FLEXPIPE_NEXT_RANKS: [4] |     RANKS_IN_EACH_PIPELINE_STAGE: [[0, 1, 2, 3, 4, 5, 6, 7]]|     OP_RESHARDING_RANKS: [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] |     EMBEDDING_GLOBAL_RANKS: range(0, 1) |     POSITION_EMBEDDING_GLOBAL_RANKS: range(0, 1)
time to initialize megatron (seconds): 22.306
[after megatron is initialized] datetime: 2024-12-19 20:16:22 
building GPT model ...
[rank 1 all ops] "dec-embedding","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-post-process",
[rank 1 recompute ops] 
[rank 7 all ops] "dec-embedding","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-post-process",
[rank 7 recompute ops] 
[rank 2 all ops] "dec-embedding","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-post-process",
[rank 2 recompute ops] 
[rank 5 all ops] "dec-embedding","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-post-process",
[rank 5 recompute ops] 
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
[rank 0 all ops] "dec-embedding","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-post-process",
[rank 0 recompute ops] 
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
[rank 4 all ops] "dec-embedding","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-post-process",
[rank 4 recompute ops] 
[rank 6 all ops] "dec-embedding","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-post-process",
[rank 6 recompute ops] 
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1526059008
[rank 3 all ops] "dec-embedding","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-self-attention","dec-mlp","dec-post-process",
[rank 3 recompute ops] 
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 4
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (4194304 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.49.embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 2 (103022592 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.49.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 3 (1315819520 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.48.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.35.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.29.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.23.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.8.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.43.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.30.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.24.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.20.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.16.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.10.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.4.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.45.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.32.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.26.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.18.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.5.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.46.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.40.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.34.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.21.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.13.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.7.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.1.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.41.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.35.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.29.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.15.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.9.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.43.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.37.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.31.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.10.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.4.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.38.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.32.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.26.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.18.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.12.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.6.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.46.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.40.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.27.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.7.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.1.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.48.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.42.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.29.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.23.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.15.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.43.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.37.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.24.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.20.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.4.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.39.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.45.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.26.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.12.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.40.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.34.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.28.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.13.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.7.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.48.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.35.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.29.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.23.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.15.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.9.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.37.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.31.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.25.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.21.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.17.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.10.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.4.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.6.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.45.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.32.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.26.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.18.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.12.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.46.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.40.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.34.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.21.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.14.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.48.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.42.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.36.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.23.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.15.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.9.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.43.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.37.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.31.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.17.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.11.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.45.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.39.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.33.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.19.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.12.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.6.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.7.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.47.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.34.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.28.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.22.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.3.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.48.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.42.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.29.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.23.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.9.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.44.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.31.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.25.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.20.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.17.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.4.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.position_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.45.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.39.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.26.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.6.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.8.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.40.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.34.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.28.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.14.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.42.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.36.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.30.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.15.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.9.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.3.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.37.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.31.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.25.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.20.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.17.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.11.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.5.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.39.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.33.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.27.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.19.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.12.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.6.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.47.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.41.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.28.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.22.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.14.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.48.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.42.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.36.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.23.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.16.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.38.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.44.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.25.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.17.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.11.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.45.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.39.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.33.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.19.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.13.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.47.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.34.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.28.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.22.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.14.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.8.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.36.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.30.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.24.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.9.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.44.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.31.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.25.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.17.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.11.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.5.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.46.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.33.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.27.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.19.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.6.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.47.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.41.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.35.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.22.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.14.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.8.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.42.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.36.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.30.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.16.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.10.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.44.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.38.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.32.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.18.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.11.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.5.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.13.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.39.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.33.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.27.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.21.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.19.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.7.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.47.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.41.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.28.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.22.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.8.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.2.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.49.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.43.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.30.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.24.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.16.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.44.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.38.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.25.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.21.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.5.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.46.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.40.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.27.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.13.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.41.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.35.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.29.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.14.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.8.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.2.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.49.final_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.36.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.30.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.24.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.20.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.16.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.10.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.4.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.5.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.38.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.32.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.26.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.21.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.18.pre_mlp_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.11.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.7.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.46.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.33.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.27.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.19.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.13.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.47.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.41.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.35.input_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.22.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.15.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.43.input_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.37.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.24.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.20.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.16.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.10.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.44.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.38.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.32.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.18.pre_mlp_layernorm.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.language_model.ops.12.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 4 (103022592 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig(optimizer='adam', lr=6e-05, min_lr=6e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4096.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0xffff7d9b26a0>)
> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-12-19 20:16:23 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      18432
    validation: 1024
    test:       1024
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = True
> building train, validation, and test datasets for GPT ...
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-12-19 20:16:25 
done with setup ...
/workspace/Ascend-aceso/megatron/core/timers.py:241: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  rank_name_to_time[rank, i] = self._timers[name].elapsed(reset=reset)
/workspace/Ascend-aceso/megatron/core/timers.py:241: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  rank_name_to_time[rank, i] = self._timers[name].elapsed(reset=reset)
/workspace/Ascend-aceso/megatron/core/timers.py:241: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  rank_name_to_time[rank, i] = self._timers[name].elapsed(reset=reset)
/workspace/Ascend-aceso/megatron/core/timers.py:241: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  rank_name_to_time[rank, i] = self._timers[name].elapsed(reset=reset)
/workspace/Ascend-aceso/megatron/core/timers.py:241: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  rank_name_to_time[rank, i] = self._timers[name].elapsed(reset=reset)
/workspace/Ascend-aceso/megatron/core/timers.py:241: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  rank_name_to_time[rank, i] = self._timers[name].elapsed(reset=reset)
/workspace/Ascend-aceso/megatron/core/timers.py:241: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  rank_name_to_time[rank, i] = self._timers[name].elapsed(reset=reset)
/workspace/Ascend-aceso/megatron/core/timers.py:241: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  rank_name_to_time[rank, i] = self._timers[name].elapsed(reset=reset)
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (616.23, 700.78)
    train/valid/test-data-iterators-setup ..........: (175.52, 2031.35)

==> Time (ms) | [stage 0, virtual 0, rank 1] | model-and-optimizer-setup: 0.00 | train/valid/test-data-iterators-setup: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 6549.16015625 | max allocated: 17408.0244140625 | reserved: 8542.0 | max reserved: 17604.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | model-and-optimizer-setup: 0.00 | train/valid/test-data-iterators-setup: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 6549.150390625 | max allocated: 17408.0244140625 | reserved: 8542.0 | max reserved: 17604.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | model-and-optimizer-setup: 0.00 | train/valid/test-data-iterators-setup: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 6549.16064453125 | max allocated: 17408.0244140625 | reserved: 8542.0 | max reserved: 17604.0
training ...

==> Time (ms) | [stage 0, virtual 0, rank 6] | model-and-optimizer-setup: 0.00 | train/valid/test-data-iterators-setup: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 6549.1611328125 | max allocated: 17408.0244140625 | reserved: 8542.0 | max reserved: 17604.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | model-and-optimizer-setup: 0.00 | train/valid/test-data-iterators-setup: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 6549.162109375 | max allocated: 17408.0244140625 | reserved: 8542.0 | max reserved: 17604.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | model-and-optimizer-setup: 0.00 | train/valid/test-data-iterators-setup: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 6549.1591796875 | max allocated: 17408.0244140625 | reserved: 8542.0 | max reserved: 17604.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | model-and-optimizer-setup: 0.00 | train/valid/test-data-iterators-setup: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 6549.16015625 | max allocated: 17408.0244140625 | reserved: 8542.0 | max reserved: 17604.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | model-and-optimizer-setup: 0.00 | train/valid/test-data-iterators-setup: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 6549.1611328125 | max allocated: 17408.0244140625 | reserved: 8542.0 | max reserved: 17604.0
[before the start of training step] datetime: 2024-12-19 20:16:25 
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.

Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.
Warning: Device do not support double dtype now, dtype cast repalce with float.

Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.Warning: Device do not support double dtype now, dtype cast repalce with float.


[W compiler_depend.ts:103] Warning: Non finite check and unscale on NPU device! (function operator())
[W compiler_depend.ts:103] Warning: Non finite check and unscale on NPU device! (function operator())
[W compiler_depend.ts:103] Warning: Non finite check and unscale on NPU device! (function operator())
[W compiler_depend.ts:103] Warning: Non finite check and unscale on NPU device! (function operator())
[W compiler_depend.ts:103] Warning: Non finite check and unscale on NPU device! (function operator())
[W compiler_depend.ts:103] Warning: Non finite check and unscale on NPU device! (function operator())
[W compiler_depend.ts:103] Warning: Non finite check and unscale on NPU device! (function operator())
[W compiler_depend.ts:103] Warning: Non finite check and unscale on NPU device! (function operator())
 [2024-12-19 20:16:42] iteration        1/      18 | consumed samples:         8192 | elapsed time per iteration (ms): 17535.8 | throughput per GPU (TFLOP/s/GPU): 135.6 | learning rate: 1.875000E-07 | global batch size:  1024 | lm loss: 9.924793E+00 | loss scale: 4096.0 | grad norm: 434.221 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 23223.0068359375 | reserved: 23706.0 | max reserved: 23706.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14852.99, 14857.95)
    forward-compute ................................: (5471.12, 5589.54)
    backward-compute ...............................: (9237.77, 9357.58)
    batch-generator ................................: (280.61, 292.30)
    layernorm-grads-all-reduce .....................: (0.08, 0.12)
    embedding-grads-all-reduce .....................: (0.06, 0.10)
    all-grads-sync .................................: (132.19, 132.62)
    params-all-gather ..............................: (87.36, 87.68)
    optimizer-copy-to-main-grad ....................: (2.06, 3.76)
    optimizer-unscale-and-check-inf ................: (1634.53, 1634.99)
    optimizer-clip-main-grad .......................: (24.81, 25.32)
    optimizer-count-zeros ..........................: (0.06, 0.07)
    optimizer-inner-step ...........................: (713.36, 769.47)
    optimizer-copy-main-to-model-params ............: (1.84, 3.80)
    optimizer ......................................: (2528.38, 2528.70)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 23223.00634765625 | reserved: 23686.0 | max reserved: 23686.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 23223.00830078125 | reserved: 23706.0 | max reserved: 23706.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 23222.99658203125 | reserved: 23684.0 | max reserved: 23684.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 23223.0068359375 | reserved: 23706.0 | max reserved: 23706.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 23223.00537109375 | reserved: 23686.0 | max reserved: 23686.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 23223.00634765625 | reserved: 23686.0 | max reserved: 23686.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 23223.00732421875 | reserved: 23686.0 | max reserved: 23686.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 23223.00732421875 | reserved: 23686.0 | max reserved: 23686.0
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:16:56] iteration        2/      18 | consumed samples:        16384 | elapsed time per iteration (ms): 14198.8 | throughput per GPU (TFLOP/s/GPU): 167.5 | learning rate: 3.750000E-07 | global batch size:  1024 | lm loss: 9.924611E+00 | loss scale: 4096.0 | grad norm: 465.562 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 2 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25326.0 | max reserved: 25326.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14094.40, 14100.35)
    forward-compute ................................: (4765.98, 4916.50)
    backward-compute ...............................: (9147.74, 9305.67)
    batch-generator ................................: (36.32, 42.37)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.07, 0.07)
    all-grads-sync .................................: (28.47, 28.52)
    params-all-gather ..............................: (21.71, 21.77)
    optimizer-copy-to-main-grad ....................: (1.19, 1.86)
    optimizer-unscale-and-check-inf ................: (2.06, 2.13)
    optimizer-clip-main-grad .......................: (5.98, 6.55)
    optimizer-count-zeros ..........................: (0.05, 0.09)
    optimizer-inner-step ...........................: (6.00, 6.32)
    optimizer-copy-main-to-model-params ............: (1.23, 1.65)
    optimizer ......................................: (43.20, 43.27)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25386.0 | max reserved: 25386.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:17:11] iteration        3/      18 | consumed samples:        24576 | elapsed time per iteration (ms): 14282.6 | throughput per GPU (TFLOP/s/GPU): 166.5 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 4096.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 3 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14216.52, 14225.79)
    forward-compute ................................: (4807.43, 5035.10)
    backward-compute ...............................: (9146.70, 9390.82)
    batch-generator ................................: (35.18, 41.66)
    layernorm-grads-all-reduce .....................: (0.07, 0.09)
    embedding-grads-all-reduce .....................: (0.07, 0.07)
    all-grads-sync .................................: (28.45, 28.55)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.17, 1.96)
    optimizer-unscale-and-check-inf ................: (2.05, 2.12)
    optimizer ......................................: (5.40, 5.44)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25386.0 | max reserved: 25386.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0


==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:17:25] iteration        4/      18 | consumed samples:        32768 | elapsed time per iteration (ms): 14398.7 | throughput per GPU (TFLOP/s/GPU): 165.2 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 2048.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 4 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14326.58, 14336.93)
    forward-compute ................................: (4863.86, 5152.62)
    backward-compute ...............................: (9143.82, 9439.49)
    batch-generator ................................: (33.85, 39.73)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.07, 0.08)
    all-grads-sync .................................: (28.55, 28.66)
    params-all-gather ..............................: (0.08, 0.08)
    optimizer-copy-to-main-grad ....................: (1.18, 1.90)
    optimizer-unscale-and-check-inf ................: (2.04, 2.09)
    optimizer ......................................: (11.10, 11.21)

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0


==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:17:40] iteration        5/      18 | consumed samples:        40960 | elapsed time per iteration (ms): 14397.1 | throughput per GPU (TFLOP/s/GPU): 165.2 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 1024.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 5 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14331.81, 14341.57)
    forward-compute ................................: (4873.00, 5157.20)
    backward-compute ...............................: (9145.85, 9439.58)
    batch-generator ................................: (33.50, 39.54)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.07, 0.07)
    all-grads-sync .................................: (28.41, 28.47)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.16, 2.58)
    optimizer-unscale-and-check-inf ................: (1.98, 2.03)
    optimizer ......................................: (6.06, 6.08)

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:17:54] iteration        6/      18 | consumed samples:        49152 | elapsed time per iteration (ms): 14431.3 | throughput per GPU (TFLOP/s/GPU): 164.8 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 512.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 6 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14366.74, 14375.23)
    forward-compute ................................: (4923.65, 5191.82)
    backward-compute ...............................: (9144.99, 9422.47)
    batch-generator ................................: (33.14, 40.61)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.69, 28.78)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.13, 1.93)
    optimizer-unscale-and-check-inf ................: (1.94, 1.99)
    optimizer ......................................: (5.37, 5.40)

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:18:08] iteration        7/      18 | consumed samples:        57344 | elapsed time per iteration (ms): 14379.3 | throughput per GPU (TFLOP/s/GPU): 165.4 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 256.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 7 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14315.81, 14323.25)
    forward-compute ................................: (4932.59, 5141.61)
    backward-compute ...............................: (9146.57, 9362.29)
    batch-generator ................................: (33.53, 38.80)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.51, 28.60)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.14, 1.85)
    optimizer-unscale-and-check-inf ................: (1.96, 2.01)
    optimizer ......................................: (5.21, 5.27)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0
==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0


==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:18:23] iteration        8/      18 | consumed samples:        65536 | elapsed time per iteration (ms): 14353.2 | throughput per GPU (TFLOP/s/GPU): 165.7 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 128.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 8 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14294.57, 14298.24)
    forward-compute ................................: (4958.23, 5118.33)
    backward-compute ...............................: (9146.97, 9310.59)
    batch-generator ................................: (33.52, 39.53)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.37, 28.46)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.15, 1.99)
    optimizer-unscale-and-check-inf ................: (1.95, 2.00)
    optimizer ......................................: (5.47, 5.54)

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:18:37] iteration        9/      18 | consumed samples:        73728 | elapsed time per iteration (ms): 14323.7 | throughput per GPU (TFLOP/s/GPU): 166.1 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 64.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 9 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14263.12, 14268.48)
    forward-compute ................................: (4970.46, 5089.77)
    backward-compute ...............................: (9149.58, 9269.50)
    batch-generator ................................: (32.60, 38.91)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.59, 28.67)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.16, 1.88)
    optimizer-unscale-and-check-inf ................: (2.01, 2.06)
    optimizer ......................................: (5.33, 5.36)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0
==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0


==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}



0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:18:52] iteration       10/      18 | consumed samples:        81920 | elapsed time per iteration (ms): 14854.6 | throughput per GPU (TFLOP/s/GPU): 160.1 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 32.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 10 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14795.64, 14799.23)
    forward-compute ................................: (5562.54, 5624.83)
    backward-compute ...............................: (9144.63, 9208.20)
    batch-generator ................................: (34.85, 40.51)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.07, 0.07)
    all-grads-sync .................................: (28.22, 28.27)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.15, 1.99)
    optimizer-unscale-and-check-inf ................: (1.98, 2.04)
    optimizer ......................................: (5.45, 5.48)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:19:06] iteration       11/      18 | consumed samples:        90112 | elapsed time per iteration (ms): 14224.9 | throughput per GPU (TFLOP/s/GPU): 167.2 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 16.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 11 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14163.86, 14164.88)
    forward-compute ................................: (4950.43, 5000.96)
    backward-compute ...............................: (9137.73, 9186.56)
    batch-generator ................................: (34.25, 40.90)
    layernorm-grads-all-reduce .....................: (0.08, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.24, 28.41)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.18, 5.73)
    optimizer-unscale-and-check-inf ................: (1.99, 2.05)
    optimizer ......................................: (9.37, 9.43)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:19:20] iteration       12/      18 | consumed samples:        98304 | elapsed time per iteration (ms): 14189.9 | throughput per GPU (TFLOP/s/GPU): 167.6 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 8.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 12 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14130.83, 14132.10)
    forward-compute ................................: (4937.91, 4978.75)
    backward-compute ...............................: (9125.89, 9165.90)
    batch-generator ................................: (33.80, 40.35)
    layernorm-grads-all-reduce .....................: (0.08, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.10)
    all-grads-sync .................................: (28.63, 28.70)
    params-all-gather ..............................: (0.07, 0.10)
    optimizer-copy-to-main-grad ....................: (1.14, 2.27)
    optimizer-unscale-and-check-inf ................: (2.29, 2.32)
    optimizer ......................................: (6.10, 6.18)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:19:34] iteration       13/      18 | consumed samples:       106496 | elapsed time per iteration (ms): 14194.8 | throughput per GPU (TFLOP/s/GPU): 167.6 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 4.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 13 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14134.91, 14137.52)
    forward-compute ................................: (4946.64, 4978.41)
    backward-compute ...............................: (9127.80, 9162.94)
    batch-generator ................................: (33.13, 40.57)
    layernorm-grads-all-reduce .....................: (0.08, 0.09)
    embedding-grads-all-reduce .....................: (0.07, 0.07)
    all-grads-sync .................................: (28.64, 28.82)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.15, 1.86)
    optimizer-unscale-and-check-inf ................: (2.01, 2.04)
    optimizer ......................................: (5.33, 5.38)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:19:49] iteration       14/      18 | consumed samples:       114688 | elapsed time per iteration (ms): 14132.0 | throughput per GPU (TFLOP/s/GPU): 168.3 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 2.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 14 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14075.06, 14076.52)
    forward-compute ................................: (4884.45, 4932.69)
    backward-compute ...............................: (9114.05, 9164.57)
    batch-generator ................................: (33.50, 40.32)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.56, 28.69)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.17, 1.85)
    optimizer-unscale-and-check-inf ................: (1.95, 2.02)
    optimizer ......................................: (5.32, 5.36)

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:20:03] iteration       15/      18 | consumed samples:       122880 | elapsed time per iteration (ms): 14111.2 | throughput per GPU (TFLOP/s/GPU): 168.6 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 1.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 15 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (14053.59, 14055.48)
    forward-compute ................................: (4866.05, 4913.77)
    backward-compute ...............................: (9112.37, 9162.34)
    batch-generator ................................: (33.59, 41.52)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.59, 28.71)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.17, 1.89)
    optimizer-unscale-and-check-inf ................: (1.95, 2.00)
    optimizer ......................................: (5.31, 5.36)

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:20:17] iteration       16/      18 | consumed samples:       131072 | elapsed time per iteration (ms): 13985.1 | throughput per GPU (TFLOP/s/GPU): 170.1 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 1.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 16 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (13927.50, 13929.02)
    forward-compute ................................: (4738.78, 4791.13)
    backward-compute ...............................: (9108.98, 9163.46)
    batch-generator ................................: (33.27, 41.59)
    layernorm-grads-all-reduce .....................: (0.08, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.55, 28.64)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.17, 1.92)
    optimizer-unscale-and-check-inf ................: (1.96, 2.02)
    optimizer ......................................: (5.35, 5.39)

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:20:31] iteration       17/      18 | consumed samples:       139264 | elapsed time per iteration (ms): 13989.2 | throughput per GPU (TFLOP/s/GPU): 170.0 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 1.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 17 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (13930.21, 13932.38)
    forward-compute ................................: (4741.36, 4792.48)
    backward-compute ...............................: (9110.21, 9162.10)
    batch-generator ................................: (33.30, 40.27)
    layernorm-grads-all-reduce .....................: (0.08, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.08)
    all-grads-sync .................................: (28.81, 29.00)
    params-all-gather ..............................: (0.07, 0.08)
    optimizer-copy-to-main-grad ....................: (1.16, 2.01)
    optimizer-unscale-and-check-inf ................: (2.00, 2.06)
    optimizer ......................................: (5.80, 5.88)

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00
==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00


==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0
==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0


==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}7, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

1, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}

4, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}


5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
6, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}3, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}



5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
0, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
5, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {49: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
2, op.shared_weights_info[key]["sharing_weights_in_same_pipeline_rank"]: {0: True}, op.shared_weights_info[key]["sharing_weights_with_ranks": {}
 [2024-12-19 20:20:45] iteration       18/      18 | consumed samples:       147456 | elapsed time per iteration (ms): 13968.3 | throughput per GPU (TFLOP/s/GPU): 170.3 | learning rate: 3.750000E-07 | global batch size:  1024 | loss scale: 1.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 18 iterations) memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (13909.79, 13911.95)
    forward-compute ................................: (4720.18, 4774.91)
    backward-compute ...............................: (9107.96, 9164.37)
    batch-generator ................................: (32.68, 40.57)
    layernorm-grads-all-reduce .....................: (0.07, 0.08)
    embedding-grads-all-reduce .....................: (0.06, 0.07)
    all-grads-sync .................................: (28.56, 28.69)
    params-all-gather ..............................: (0.07, 0.10)
    optimizer-copy-to-main-grad ....................: (1.15, 1.96)
    optimizer-unscale-and-check-inf ................: (2.03, 2.07)
    optimizer ......................................: (5.48, 5.51)

==> Time (ms) | [stage 0, virtual 0, rank 1] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 1] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 3] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 3] memory (MB) | allocated: 8932.7763671875 | max allocated: 24874.91796875 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 6] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 6] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0

==> Time (ms) | [stage 0, virtual 0, rank 7] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 7] memory (MB) | allocated: 8932.7294921875 | max allocated: 24874.8828125 | reserved: 25426.0 | max reserved: 25426.0

==> Time (ms) | [stage 0, virtual 0, rank 4] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 4] memory (MB) | allocated: 8932.7646484375 | max allocated: 24874.9091796875 | reserved: 25366.0 | max reserved: 25366.0

==> Time (ms) | [stage 0, virtual 0, rank 0] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 0] memory (MB) | allocated: 8932.7705078125 | max allocated: 24874.91357421875 | reserved: 25346.0 | max reserved: 25346.0

==> Time (ms) | [stage 0, virtual 0, rank 5] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 5] memory (MB) | allocated: 8932.7685546875 | max allocated: 24874.912109375 | reserved: 25328.0 | max reserved: 25328.0

==> Time (ms) | [stage 0, virtual 0, rank 2] | forward-backward: 0.00 | forward-compute: 0.00 | backward-compute: 0.00 | batch-generator: 0.00 | layernorm-grads-all-reduce: 0.00 | embedding-grads-all-reduce: 0.00 | all-grads-sync: 0.00 | params-all-gather: 0.00 | optimizer-copy-to-main-grad: 0.00 | optimizer-unscale-and-check-inf: 0.00 | optimizer-clip-main-grad: 0.00 | optimizer-count-zeros: 0.00 | optimizer-inner-step: 0.00 | optimizer-copy-main-to-model-params: 0.00 | optimizer: 0.00

==> Memory | [stage 0, virtual 0, rank 2] memory (MB) | allocated: 8932.7724609375 | max allocated: 24874.9150390625 | reserved: 25326.0 | max reserved: 25326.0
[after training is done] datetime: 2024-12-19 20:20:45 
